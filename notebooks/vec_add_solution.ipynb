{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SICxTMkZg4Eh"
   },
   "source": [
    "# What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47qfI-M8L46r",
    "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
   },
   "outputs": [],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSb5bpl6g-Iz"
   },
   "source": [
    "# Vector Addition Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-2GrrEMNPLa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "# The CUDA kernel\n",
    "# Goal: add 5 to a vector of zeros\n",
    "# in:  0 0 0 0 0 0...\n",
    "# out: 5 5 5 5 5 5...\n",
    "@cuda.jit\n",
    "def vec_add_constant(x, n, c):\n",
    "    # thread position in the grid\n",
    "    # shortcut for: i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    # Make sure we stop computing at n. \n",
    "    # If n is not a multiple of threads_per_blocks, \n",
    "    # it is likely that we have an extra block with empty values, some threads will have to wait\n",
    "    if i < n:\n",
    "        x[i] += c\n",
    "        \n",
    "n = 4096 # vector size\n",
    "c = 5.0 # constant to be added\n",
    "\n",
    "h_x = np.zeros(n, dtype=np.float32) # host array filled with zeros\n",
    "print(h_x)\n",
    "\n",
    "# copy h_x to a device array named d_x on the GPU\n",
    "d_x = cuda.to_device(h_x)\n",
    "\n",
    "threads_per_block = 32 # between 32 and 1024\n",
    "\n",
    "# compute the number of blocks necessary to solve the problem: 1024 / 256 = 4\n",
    "blocks = (n + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# call the CUDA kernel on the GPU\n",
    "vec_add_constant[blocks, threads_per_block](d_x, n, c)\n",
    "\n",
    "# copy d_x back to h_x (from GPU to CPU)\n",
    "h_x = d_x.copy_to_host()\n",
    "\n",
    "print(h_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-acKLc1ksNt"
   },
   "source": [
    "## Exercise 1: Vector Addition\n",
    "\n",
    "Now it's your turn to implement the CUDA kernel! \n",
    "\n",
    "The goal is to add two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XE-RxFQRQ29"
   },
   "outputs": [],
   "source": [
    "# CUDA kernel to add two vectors\n",
    "# in:\n",
    "# a:  0 0 0 0 0\n",
    "# b:  5 5 5 5 5\n",
    "# out:\n",
    "# c:  5 5 5 5 5\n",
    "@cuda.jit\n",
    "def vec_add(a, b, c, n):\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i < n:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "n = 4096 # vector size\n",
    "c = 5.0 # constant to be added\n",
    "\n",
    "h_a = np.zeros(n, dtype=np.float32) # host array filled with zeros\n",
    "h_b = np.full(n, 5.0, dtype=np.float32) # host array filled with 5s\n",
    "h_c = np.zeros(n, dtype=np.float32) # host array filled with zeros\n",
    "\n",
    "print(h_a)\n",
    "print(h_b)\n",
    "\n",
    "# copy h to d\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_b = cuda.to_device(h_b)\n",
    "d_c = cuda.to_device(h_c)\n",
    "\n",
    "threads_per_block = 32 # between 32 and 1024\n",
    "\n",
    "# compute the number of blocks necessary to solve the problem: 1024 / 256 = 4\n",
    "blocks = (n + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "# call the CUDA kernel on the GPU\n",
    "vec_add[blocks, threads_per_block](d_a, d_b, d_c, n)\n",
    "\n",
    "# copy d to h\n",
    "h_a = d_a.copy_to_host()\n",
    "h_b = d_b.copy_to_host()\n",
    "h_c = d_c.copy_to_host()\n",
    "\n",
    "print(h_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibfklGMzlxgI",
    "tags": []
   },
   "source": [
    "## Exercise 2: Simple Memory Management\n",
    "\n",
    "By default, if we let Numba take care of the data transfers, Numba will copy all three arrays to and from the device everytime. \n",
    "\n",
    "This would be a good time to do some profiling using nvprof:\n",
    "```\n",
    "==8912== Profiling application: python vec_add.py\n",
    "==8912== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:   61.98%  79.743ms         6  13.290ms  5.6272ms  28.232ms  [CUDA memcpy DtoH]\n",
    "                   36.66%  47.159ms         6  7.8599ms  5.3962ms  12.592ms  [CUDA memcpy HtoD]\n",
    "                    1.36%  1.7535ms         2  876.77us  876.74us  876.80us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
    "      API calls:   39.09%  83.395ms         6  13.899ms  5.7020ms  29.060ms  cuMemcpyDtoH\n",
    "                   38.00%  81.068ms         1  81.068ms  81.068ms  81.068ms  cuDevicePrimaryCtxRetain\n",
    "                   22.22%  47.419ms         6  7.9031ms  5.3943ms  12.724ms  cuMemcpyHtoD\n",
    "```\n",
    "\n",
    "**61.98**% of the total time is spent in the **[CUDA memcpy DtoH]** function, and **[CUDA memcpy HtoD]** function. **In total, 98.6% of the total execution time on the GPU is lost in data transfers...** Yes, only 1.36% of time is calculations.\n",
    "\n",
    "But in fact, we don't need to copy all three arrays everytime. We need to copy array a and b **to** the device (c will be set on the device), and we need to copy array c **from** the device to get the results.\n",
    "\n",
    "Below are some examples of how to control data transfers manually:\n",
    "\n",
    "```\n",
    "# Create device array d_a from array a and copy it to the device\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Alternatively, create device array d_c from array c but DON'T copy it\n",
    "d_c = cuda.to_device(c, copy=False)\n",
    "\n",
    "# Copy the content of device array d_c to host array c\n",
    "d_c.copy_to_host(c)\n",
    "``` \n",
    "\n",
    "Then when calling the kernel function, use the freshly created device arrays rather than the host arrays:\n",
    "```\n",
    "vec_add[gridsize, blocksize](d_a, d_b, d_c)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB6P8qdittRC"
   },
   "source": [
    "\n",
    "If you did it right, it should be significantly faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_add(a, b, c, n):\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i < n:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "n = 4096\n",
    "c = 5.0\n",
    "\n",
    "h_a = np.zeros(n, dtype=np.float32)\n",
    "h_b = np.full(n, 5.0, dtype=np.float32)\n",
    "h_c = np.empty(n, dtype=np.float32) # EDIT: empty array, no need to init!\n",
    "\n",
    "print(h_a)\n",
    "print(h_b)\n",
    "\n",
    "# copy h to d\n",
    "d_a = cuda.to_device(h_a)\n",
    "d_b = cuda.to_device(h_b)\n",
    "d_c = cuda.to_device(h_c, copy=False) # EDIT: no need to copy c!\n",
    "\n",
    "threads_per_block = 32\n",
    "blocks = (n + threads_per_block - 1) // threads_per_block\n",
    "vec_add[blocks, threads_per_block](d_a, d_b, d_c, n)\n",
    "\n",
    "# h_a = d_a.copy_to_host() # EDIT: we only need c!\n",
    "# h_b = d_b.copy_to_host() # EDIT: we only need c!\n",
    "h_c = d_c.copy_to_host() # EDIT: we only need c!\n",
    "\n",
    "print(h_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > vec_add.cu << 'EOF'\n",
    "\n",
    "#include <cuda_runtime.h>\n",
    "#include <iostream>\n",
    "\n",
    "__global__ void vec_add(const float* a, const float* b, float* c, int n)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)\n",
    "        c[i] = a[i] + b[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int n = 4096;\n",
    "\n",
    "    // --- host arrays ---\n",
    "    float* h_a = new float[n];          // zero-initialized\n",
    "    float* h_b = new float[n];\n",
    "    float* h_c = new float[n];            // uninitialized (like np.empty)\n",
    "\n",
    "    for (int i = 0; i < n; ++i)\n",
    "    {\n",
    "        h_a[i] = 0.0f;\n",
    "        h_b[i] = 5.0f;\n",
    "    }\n",
    "\n",
    "    // --- device arrays ---\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, n * sizeof(float));\n",
    "    cudaMalloc(&d_b, n * sizeof(float));\n",
    "    cudaMalloc(&d_c, n * sizeof(float));\n",
    "\n",
    "    // copy inputs only\n",
    "    cudaMemcpy(d_a, h_a, n * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, n * sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // launch\n",
    "    int threads = 32;\n",
    "    int blocks  = (n + threads - 1) / threads;\n",
    "    vec_add<<<blocks, threads>>>(d_a, d_b, d_c, n);\n",
    "\n",
    "    // copy result only\n",
    "    cudaMemcpy(h_c, d_c, n * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // print first few values\n",
    "    for (int i = 0; i < 10; ++i)\n",
    "        std::cout << h_c[i] << \" \";\n",
    "    std::cout << std::endl;\n",
    "../../../notebooks/vec_add_solutions.ipynb\n",
    "    // cleanup\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    delete[] h_a;\n",
    "    delete[] h_b;\n",
    "    delete[] h_c;\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc vec_add.cu -O2 -o vec_add\n",
    "./vec_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vec_add_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gpu-course)",
   "language": "python",
   "name": "gpu-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442ebfd9",
   "metadata": {},
   "source": [
    "# computing $\\pi$ with GPU\n",
    "\n",
    "The goal of this exercice is to compute an estimate of $\\pi$ on the GPU using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration).\n",
    "\n",
    "\n",
    "This technique consists in generating a number random 2D points with coordinates ranging from 0 to 1.\n",
    "and then the distance of these points to the origin.\n",
    " \n",
    "The fraction of points falling whithin a distance 1.0 from the origin should approximate $\\pi/4$ with an increasing precision as the number of generated points increase.\n",
    "\n",
    "\n",
    "![](figures/MCpi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e4a40",
   "metadata": {
    "id": "SICxTMkZg4Eh"
   },
   "source": [
    "### What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6deb94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47qfI-M8L46r",
    "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU:\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "model name\t: 13th Gen Intel(R) Core(TM) i7-13700K\n",
      "GPU:\n",
      "Tue Feb 11 12:10:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.16              Driver Version: 570.86.16      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   42C    P5             34W /  450W |    1708MiB /  24564MiB |     12%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1444      G   /usr/bin/gnome-shell                    399MiB |\n",
      "|    0   N/A  N/A            1913    C+G   .../lib/xdg-desktop-portal-gnome         41MiB |\n",
      "|    0   N/A  N/A            2162      G   /usr/bin/Xwayland                         8MiB |\n",
      "|    0   N/A  N/A            2199      G   ...and --variations-seed-version         94MiB |\n",
      "|    0   N/A  N/A            2385      G   /usr/lib/firefox/firefox                213MiB |\n",
      "|    0   N/A  N/A            8669      C   ...iniforge3/envs/fit/bin/python        402MiB |\n",
      "|    0   N/A  N/A            9449      G   /opt/zoom/zoom                          263MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6fb6",
   "metadata": {
    "id": "pSb5bpl6g-Iz"
   },
   "source": [
    "## CPU implementation\n",
    "\n",
    "We provide a standard Python implementation for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ad9b0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14272"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import random\n",
    "from numba import jit,njit,prange,cuda, types, float32\n",
    "\n",
    "@njit()\n",
    "def estimate_pi_cpu( nb_points ):\n",
    "    nb_points_in = 0 # number of points in the circle\n",
    "    \n",
    "    for i in range(nb_points):\n",
    "        pt = random(2) # random 2D coordinates between 0 and 1\n",
    "        \n",
    "        dist = (pt**2).sum()**0.5  # distance from the origin\n",
    "        \n",
    "        nb_points_in += (dist <= 1.0) # increment if distance is <= 1\n",
    "\n",
    "    return (nb_points_in / nb_points)*4\n",
    "    \n",
    "\n",
    "# a single estimate of pi\n",
    "estimate_pi_cpu( 10**6 )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6143b",
   "metadata": {
    "id": "hjyY1tO_h8hp"
   },
   "source": [
    "The complexity is linear with the number of points generated. \n",
    "Therefore here we use a large vector size to increase the execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3da572bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 526 ms, sys: 1.8 ms, total: 527 ms\n",
      "Wall time: 527 ms\n",
      "3.1420748233795166\n"
     ]
    }
   ],
   "source": [
    "# timing the estimate with 2**24 points:\n",
    "%time pi_estimate = estimate_pi_cpu( 2**24 ) \n",
    "print( pi_estimate )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e100a3",
   "metadata": {
    "id": "UwG7GcJuh1rc",
    "tags": []
   },
   "source": [
    "## The CUDA implementation\n",
    "\n",
    "Now it's your turn to implement the CUDA kernel! \n",
    "\n",
    "The main difficulties here reside in the implementation of the random number generation as well as the reduction to the final value. Use the [numba documentation on the topic](https://numba.readthedocs.io/en/stable/cuda/reduction.html) to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a21da4f",
   "metadata": {
    "id": "6XE-RxFQRQ29"
   },
   "outputs": [],
   "source": [
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi_kernel(states, count):\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    if idx >= states.size:\n",
    "        return\n",
    "        \n",
    "    # generate a single point and check its distance\n",
    "    x = cuda.random.xoroshiro128p_uniform_float32(states, idx)\n",
    "    y = cuda.random.xoroshiro128p_uniform_float32(states, idx)\n",
    "\n",
    "    if x * x + y * y <= 1.0:\n",
    "        # handle the reduction\n",
    "        cuda.atomic.add(count, 0, 1) # count[0] += 1\n",
    "\n",
    "def estimate_pi_gpu(nb_points):\n",
    "    blocksize = 128 # block size = number of threads per block dimension\n",
    "    gridsize = math.ceil(size / blocksize) # grid size = number of blocks per grid dimension\n",
    "    \n",
    "    states = create_xoroshiro128p_states(blocksize * gridsize, seed=1)\n",
    "    count = cuda.device_array(1, dtype=np.int32)\n",
    "    count[0] = 0\n",
    "    \n",
    "    estimate_pi_kernel[gridsize, blocksize](states, count)\n",
    "    \n",
    "    # Copy result back to host\n",
    "    inside_circle = count.copy_to_host()[0]\n",
    "    \n",
    "    # Estimate Pi\n",
    "    return (4.0 * inside_circle) / size\n",
    "\n",
    "# Why does it work so well with atomic.add? Is't this an operation that must be executed sequentially by definition???!!\n",
    "# Yes it is! But modern GPUs are heavily optimized and not all threads will perform this operation simultaneously.\n",
    "# - only ~78% of the points result in a atomic add\n",
    "# - Warp-level serialization to avoid intra-warp contention.\n",
    "# - L2 cache buffering and write combining to reduce direct global memory updates.\n",
    "# - Memory bank optimizations to avoid bank conflicts.\n",
    "# - Warp scheduling and parallel execution to hide atomic latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5d74a",
   "metadata": {},
   "source": [
    "Now, time your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3028a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 23.5 ms, total: 1.82 s\n",
      "Wall time: 1.81 s\n",
      "3.1410675048828125\n"
     ]
    }
   ],
   "source": [
    "# calling the function\n",
    "size = 2**24\n",
    "%time pi_estimate = estimate_pi_gpu(size)\n",
    "print(pi_estimate)\n",
    "\n",
    "# problem: we spend 1.78s creating 2**24 random states\n",
    "# solution: fix the number of threads and execute more points within each thread!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2820a",
   "metadata": {
    "id": "ynqVJukXjQFQ",
    "tags": []
   },
   "source": [
    "### additional tasks: Foolproof\n",
    "\n",
    "Adapt the previous code to handle sizes which are **not** a power of 2. **Hint:** you need to change both the kernel and the gridsize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "98826635-5956-4675-9fb4-bc6ed19ab90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, int32\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi_kernel(states, pts_per_thread, count):\n",
    "    idx = cuda.grid(1)  # Global thread index\n",
    "    \n",
    "    if idx >= states.size:\n",
    "        return\n",
    "\n",
    "    local_count = 0\n",
    "    for j in range(pts_per_thread):\n",
    "        # Generate two random numbers in [0,1)\n",
    "        x = xoroshiro128p_uniform_float32(states, idx)\n",
    "        y = xoroshiro128p_uniform_float32(states, idx)\n",
    "        \n",
    "        # Check if the point is inside the circle\n",
    "        if x * x + y * y <= 1.0:\n",
    "            local_count += 1\n",
    "\n",
    "    # Atomically add the threadâ€™s count into the global counter\n",
    "    cuda.atomic.add(count, 0, local_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "05d9d033-3a08-493f-b12f-88438c75cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_cuda(nb_points):\n",
    "    # Choose a fixed number of threads that is much smaller than nb_points.\n",
    "    threads_per_block = 128\n",
    "    total_threads = 2**14  # A (small) fixed number of states will be created.\n",
    "    pts_per_thread = nb_points // total_threads  # Points per thread\n",
    "\n",
    "    # Allocate a one-element array for the global count.\n",
    "    count = np.array([0], dtype=np.int32)\n",
    "    d_count = cuda.to_device(count)\n",
    "    \n",
    "    # Create one RNG state per thread (only 1024 states here).\n",
    "    states = create_xoroshiro128p_states(total_threads, seed=1)\n",
    "    \n",
    "    # Calculate the number of blocks.\n",
    "    blocks = total_threads // threads_per_block\n",
    "\n",
    "    print(\"total_threads:\", total_threads)\n",
    "    print(\"threads_per_block:\", threads_per_block)\n",
    "    print(\"pts_per_thread:\", pts_per_thread)\n",
    "    print(\"blocks:\", blocks)\n",
    "    \n",
    "    # Launch the kernel.\n",
    "    pi_kernel[blocks, threads_per_block](states, pts_per_thread, d_count)\n",
    "    \n",
    "    # Copy the result back and compute pi.\n",
    "    d_count.copy_to_host(count)\n",
    "    pi_est = 4.0 * count[0] / (pts_per_thread * total_threads)\n",
    "    return pi_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "459d9ccb-d478-4467-a455-14442ad60b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_threads: 16384\n",
      "threads_per_block: 128\n",
      "pts_per_thread: 1024\n",
      "blocks: 128\n",
      "CPU times: user 5.21 ms, sys: 1.03 ms, total: 6.24 ms\n",
      "Wall time: 5.71 ms\n",
      "3.142000913619995\n"
     ]
    }
   ],
   "source": [
    "nb_points = 2**24  # 16,777,216 total points\n",
    "%time pi_estimate = estimate_pi_cuda(nb_points)\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debc540-0d4f-4aa6-bc0b-d1a93ccb01dd",
   "metadata": {},
   "source": [
    "# Tentative reduction using shared array: not faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f2c24ffc-2c36-4840-9aea-f5305391bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, int32\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi_kernel_fixed(states, pts_per_thread, count):\n",
    "    shared = cuda.shared.array(256, int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x  # Thread index within the block\n",
    "    i = cuda.grid(1)        # Global thread index\n",
    "\n",
    "    # Each thread's local count of points inside the circle.\n",
    "    local_count = 0\n",
    "\n",
    "    # Instead of a grid-stride loop, each thread performs exactly pts_per_thread iterations.\n",
    "    for j in range(pts_per_thread):\n",
    "        x = xoroshiro128p_uniform_float32(states, i)\n",
    "        y = xoroshiro128p_uniform_float32(states, i)\n",
    "        if x*x + y*y <= 1.0:\n",
    "            local_count += 1\n",
    "\n",
    "    # Store the local result in shared memory.\n",
    "    shared[tid] = local_count\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    # Block-level reduction in shared memory.\n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if tid < stride:\n",
    "            shared[tid] += shared[tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "    # The first thread in the block adds the block's total to global_count.\n",
    "    if tid == 0:\n",
    "        cuda.atomic.add(count, 0, shared[0])\n",
    "\n",
    "def estimate_pi_cuda_opt(nb_points):\n",
    "    threads_per_block = 256\n",
    "    total_threads = 2**15  # Total number of threads (and RNG states) to create.\n",
    "    blocks = total_threads // threads_per_block  # e.g., 1024/256 = 4 blocks\n",
    "    \n",
    "    # Ensure nb_points divides evenly by total_threads.\n",
    "    pts_per_thread = nb_points // total_threads\n",
    "\n",
    "    # Allocate a one-element array on the device for the global count.\n",
    "    count = np.array([0], dtype=np.int32)\n",
    "    d_count = cuda.to_device(count)\n",
    "\n",
    "    # Create a random state per thread.\n",
    "    states = create_xoroshiro128p_states(total_threads, seed=1)\n",
    "\n",
    "    print(\"total_threads:\", total_threads)\n",
    "    print(\"threads_per_block:\", threads_per_block)\n",
    "    print(\"pts_per_thread:\", pts_per_thread)\n",
    "    print(\"blocks:\", blocks)\n",
    "\n",
    "    # Launch the kernel.\n",
    "    estimate_pi_kernel_fixed[blocks, threads_per_block](states, pts_per_thread, d_count)\n",
    "\n",
    "    # Copy the result back to host and compute pi.\n",
    "    d_count.copy_to_host(count)\n",
    "    return 4.0 * count[0] / nb_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "634e357a-7ba7-4501-a4c9-d1e3a7f2636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_threads: 32768\n",
      "threads_per_block: 256\n",
      "pts_per_thread: 65536\n",
      "blocks: 128\n",
      "CPU times: user 28.4 ms, sys: 0 ns, total: 28.4 ms\n",
      "Wall time: 27.9 ms\n",
      "3.141587231308222\n"
     ]
    }
   ],
   "source": [
    "nb_points = 2**31  # 16,777,216 total points\n",
    "%time pi_estimate = estimate_pi_cuda_opt(nb_points)\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8711b-420e-4974-8959-abca38117b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3db1bd-82c8-40d4-9a5c-6aab8dfa88f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

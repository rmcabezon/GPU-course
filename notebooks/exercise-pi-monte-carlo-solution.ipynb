{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442ebfd9",
   "metadata": {},
   "source": [
    "# computing $\\pi$ with GPU\n",
    "\n",
    "The goal of this exercice is to compute an estimate of $\\pi$ on the GPU using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration).\n",
    "\n",
    "\n",
    "This technique consists in generating a number random 2D points with coordinates ranging from 0 to 1.\n",
    "and then the distance of these points to the origin.\n",
    " \n",
    "The fraction of points falling whithin a distance 1.0 from the origin should approximate $\\pi/4$ with an increasing precision as the number of generated points increase.\n",
    "\n",
    "\n",
    "![](figures/MCpi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e4a40",
   "metadata": {
    "id": "SICxTMkZg4Eh"
   },
   "source": [
    "### What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6deb94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-05T10:36:43.744245Z",
     "iopub.status.busy": "2026-02-05T10:36:43.744056Z",
     "iopub.status.idle": "2026-02-05T10:36:44.600302Z",
     "shell.execute_reply": "2026-02-05T10:36:44.599413Z",
     "shell.execute_reply.started": "2026-02-05T10:36:43.744224Z"
    },
    "id": "47qfI-M8L46r",
    "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
   },
   "outputs": [],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6fb6",
   "metadata": {
    "id": "pSb5bpl6g-Iz"
   },
   "source": [
    "## CPU implementation\n",
    "\n",
    "We provide a standard Python implementation for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9b0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:36:56.714546Z",
     "iopub.status.busy": "2026-02-05T10:36:56.713929Z",
     "iopub.status.idle": "2026-02-05T10:37:00.561226Z",
     "shell.execute_reply": "2026-02-05T10:37:00.560441Z",
     "shell.execute_reply.started": "2026-02-05T10:36:56.714509Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import random\n",
    "from numba import jit,njit,prange,cuda, types, float32\n",
    "\n",
    "@njit()\n",
    "def estimate_pi_cpu( nb_points ):\n",
    "    nb_points_in = 0 # number of points in the circle\n",
    "    \n",
    "    for i in range(nb_points):\n",
    "        pt = random(2) # random 2D coordinates between 0 and 1\n",
    "        \n",
    "        dist = (pt**2).sum()**0.5  # distance from the origin\n",
    "        \n",
    "        nb_points_in += (dist <= 1.0) # increment if distance is <= 1\n",
    "\n",
    "    return (nb_points_in / nb_points)*4\n",
    "    \n",
    "\n",
    "# a single estimate of pi\n",
    "estimate_pi_cpu( 10**6 )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6143b",
   "metadata": {
    "id": "hjyY1tO_h8hp"
   },
   "source": [
    "The complexity is linear with the number of points generated. \n",
    "Therefore here we use a large vector size to increase the execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da572bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:37:06.211352Z",
     "iopub.status.busy": "2026-02-05T10:37:06.210544Z",
     "iopub.status.idle": "2026-02-05T10:37:07.823648Z",
     "shell.execute_reply": "2026-02-05T10:37:07.822803Z",
     "shell.execute_reply.started": "2026-02-05T10:37:06.211324Z"
    }
   },
   "outputs": [],
   "source": [
    "# timing the estimate with 2**24 points:\n",
    "%time pi_estimate = estimate_pi_cpu( 2**24 ) \n",
    "print( pi_estimate )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e100a3",
   "metadata": {
    "id": "UwG7GcJuh1rc",
    "tags": []
   },
   "source": [
    "## The CUDA implementation\n",
    "\n",
    "Now it's your turn to implement the CUDA kernel! \n",
    "\n",
    "The main difficulties here reside in the implementation of the random number generation as well as the reduction to the final value. \n",
    "\n",
    "Use the numba documentation to help you:\n",
    " * [random number generation](https://nvidia.github.io/numba-cuda/user/random.html)\n",
    " * [reduction](https://nvidia.github.io/numba-cuda/user/intrinsics.html#example) (here we propose the use of an \"atomic\" operation rather than the actual numba.reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21da4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:38:51.645122Z",
     "iopub.status.busy": "2026-02-05T10:38:51.644331Z",
     "iopub.status.idle": "2026-02-05T10:38:51.655900Z",
     "shell.execute_reply": "2026-02-05T10:38:51.655241Z",
     "shell.execute_reply.started": "2026-02-05T10:38:51.645090Z"
    },
    "id": "6XE-RxFQRQ29"
   },
   "outputs": [],
   "source": [
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi(states, count):\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    if idx >= states.size:\n",
    "        return\n",
    "        \n",
    "    # generate a single point and check its distance\n",
    "    x = cuda.random.xoroshiro128p_uniform_float32(states, idx)\n",
    "    y = cuda.random.xoroshiro128p_uniform_float32(states, idx)\n",
    "\n",
    "    if x * x + y * y <= 1.0:\n",
    "        # handle the reduction\n",
    "        cuda.atomic.add(count, 0, 1) # count[0] += 1\n",
    "\n",
    "def estimate_pi_gpu(nb_points):\n",
    "    blocksize = 128 # block size = number of threads per block dimension\n",
    "    gridsize = math.ceil(nb_points / blocksize) # grid size = number of blocks per grid dimension\n",
    "    \n",
    "    states = create_xoroshiro128p_states(blocksize * gridsize, seed=1)\n",
    "    \n",
    "    count = cuda.device_array(1, dtype=np.int32)\n",
    "    count[0] = 0\n",
    "    \n",
    "    estimate_pi[gridsize, blocksize](states, count)\n",
    "    \n",
    "    # Copy result back to host\n",
    "    inside_circle = count.copy_to_host()[0]\n",
    "    \n",
    "    # Estimate Pi\n",
    "    return (4.0 * inside_circle) / nb_points\n",
    "\n",
    "# Why does it work so well with atomic.add? Is't this an operation that must be executed sequentially by definition???!!\n",
    "# Yes it is! But modern GPUs are heavily optimized and not all threads will perform this operation simultaneously.\n",
    "# - only ~78% of the points result in a atomic add\n",
    "# - Warp-level serialization to avoid intra-warp contention.\n",
    "# - L2 cache buffering and write combining to reduce direct global memory updates.\n",
    "# - Memory bank optimizations to avoid bank conflicts.\n",
    "# - Warp scheduling and parallel execution to hide atomic latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5d74a",
   "metadata": {},
   "source": [
    "Now, time your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028a628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:40:41.232714Z",
     "iopub.status.busy": "2026-02-05T10:40:41.232339Z",
     "iopub.status.idle": "2026-02-05T10:40:46.881142Z",
     "shell.execute_reply": "2026-02-05T10:40:46.880588Z",
     "shell.execute_reply.started": "2026-02-05T10:40:41.232685Z"
    }
   },
   "outputs": [],
   "source": [
    "# calling the function\n",
    "size = 2**24\n",
    "%time pi_estimate = estimate_pi_gpu(size)\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4a722-feea-4ffc-a508-b480f28d99cd",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "We spend >90% of the time creating the 2**24 random states\n",
    "\n",
    "**Solution:** fix the number of threads.\n",
    "- We need enough threads to fill the entire GPU\n",
    "- Compute more points per threads\n",
    "- Adjust threads_per_block to have enough blocks to fill all the SMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98826635-5956-4675-9fb4-bc6ed19ab90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:50:17.341401Z",
     "iopub.status.busy": "2026-02-05T10:50:17.340782Z",
     "iopub.status.idle": "2026-02-05T10:50:17.346771Z",
     "shell.execute_reply": "2026-02-05T10:50:17.346145Z",
     "shell.execute_reply.started": "2026-02-05T10:50:17.341363Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, int32\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi_kernel(states, pts_per_thread, count):\n",
    "    idx = cuda.grid(1)  # Global thread index\n",
    "    \n",
    "    if idx >= states.size:\n",
    "        return\n",
    "\n",
    "    local_count = 0\n",
    "    for j in range(pts_per_thread):\n",
    "        # Generate two random numbers in [0,1)\n",
    "        x = xoroshiro128p_uniform_float32(states, idx)\n",
    "        y = xoroshiro128p_uniform_float32(states, idx)\n",
    "        \n",
    "        # Check if the point is inside the circle\n",
    "        if x * x + y * y <= 1.0:\n",
    "            local_count += 1\n",
    "\n",
    "    # Atomically add the threadâ€™s count into the global counter\n",
    "    cuda.atomic.add(count, 0, local_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9d033-3a08-493f-b12f-88438c75cf3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:51:06.425949Z",
     "iopub.status.busy": "2026-02-05T10:51:06.425133Z",
     "iopub.status.idle": "2026-02-05T10:51:06.431095Z",
     "shell.execute_reply": "2026-02-05T10:51:06.430440Z",
     "shell.execute_reply.started": "2026-02-05T10:51:06.425918Z"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_pi_cuda(nb_points):\n",
    "    # Choose a fixed number of threads that is much smaller than nb_points.\n",
    "    threads_per_block = 128\n",
    "    total_threads = 2**14  # A (small) fixed number of states will be created.\n",
    "    pts_per_thread = nb_points // total_threads  # Points per thread\n",
    "\n",
    "    # Allocate a one-element array for the global count.\n",
    "    count = np.array([0], dtype=np.int32)\n",
    "    d_count = cuda.to_device(count)\n",
    "    \n",
    "    # Create one RNG state per thread (only 1024 states here).\n",
    "    states = create_xoroshiro128p_states(total_threads, seed=1)\n",
    "    \n",
    "    # Calculate the number of blocks.\n",
    "    blocks = total_threads // threads_per_block\n",
    "\n",
    "    print(\"total_threads:\", total_threads)\n",
    "    print(\"threads_per_block:\", threads_per_block)\n",
    "    print(\"pts_per_thread:\", pts_per_thread)\n",
    "    print(\"blocks:\", blocks)\n",
    "    \n",
    "    # Launch the kernel.\n",
    "    estimate_pi_kernel[blocks, threads_per_block](states, pts_per_thread, d_count)\n",
    "    \n",
    "    # Copy the result back and compute pi.\n",
    "    d_count.copy_to_host(count)\n",
    "    pi_est = 4.0 * count[0] / (pts_per_thread * total_threads)\n",
    "    return pi_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d9ccb-d478-4467-a455-14442ad60b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:51:06.954572Z",
     "iopub.status.busy": "2026-02-05T10:51:06.954110Z",
     "iopub.status.idle": "2026-02-05T10:51:06.968068Z",
     "shell.execute_reply": "2026-02-05T10:51:06.967264Z",
     "shell.execute_reply.started": "2026-02-05T10:51:06.954542Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_points = 2**24  # 16,777,216 total points\n",
    "%time pi_estimate = estimate_pi_cuda(nb_points)\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd0ef2-136d-4a11-9651-91949677efd1",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5859fc4-7984-456a-bc23-1c2cd88b4495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:51:16.192337Z",
     "iopub.status.busy": "2026-02-05T10:51:16.192055Z",
     "iopub.status.idle": "2026-02-05T10:52:02.826912Z",
     "shell.execute_reply": "2026-02-05T10:52:02.826262Z",
     "shell.execute_reply.started": "2026-02-05T10:51:16.192312Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance comparison\n",
    "sizes = [2**i for i in range(10, 29)]\n",
    "gpu_times = []\n",
    "cpu_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(\"Running: \", size)\n",
    "    \n",
    "    # Measure GPU time\n",
    "    start_gpu = time.time()\n",
    "    estimate_pi_cuda(size)\n",
    "    gpu_times.append(time.time() - start_gpu)\n",
    "\n",
    "    # Measure CPU time\n",
    "    start_cpu = time.time()\n",
    "    estimate_pi_cpu(size)\n",
    "    cpu_times.append(time.time() - start_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71577d0-b95a-4a32-bf06-c2f76723175a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:52:02.828276Z",
     "iopub.status.busy": "2026-02-05T10:52:02.828043Z",
     "iopub.status.idle": "2026-02-05T10:52:03.434687Z",
     "shell.execute_reply": "2026-02-05T10:52:03.434043Z",
     "shell.execute_reply.started": "2026-02-05T10:52:02.828254Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, gpu_times, label=\"GPU\", marker='o')\n",
    "plt.plot(sizes, cpu_times, label=\"CPU\", marker='s')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of Points\")\n",
    "plt.ylabel(\"Execution Time (seconds)\")\n",
    "plt.title(\"CPU vs. GPU Execution Time for Monte Carlo Pi Estimation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debc540-0d4f-4aa6-bc0b-d1a93ccb01dd",
   "metadata": {},
   "source": [
    "# Tentative reduction using shared array: not faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c24ffc-2c36-4840-9aea-f5305391bd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:52:27.909927Z",
     "iopub.status.busy": "2026-02-05T10:52:27.909376Z",
     "iopub.status.idle": "2026-02-05T10:52:27.918755Z",
     "shell.execute_reply": "2026-02-05T10:52:27.917996Z",
     "shell.execute_reply.started": "2026-02-05T10:52:27.909896Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, int32\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "\n",
    "@cuda.jit\n",
    "def estimate_pi_kernel_fixed(states, pts_per_thread, count):\n",
    "    shared = cuda.shared.array(256, int32)\n",
    "    \n",
    "    tid = cuda.threadIdx.x  # Thread index within the block\n",
    "    i = cuda.grid(1)        # Global thread index\n",
    "\n",
    "    # Each thread's local count of points inside the circle.\n",
    "    local_count = 0\n",
    "\n",
    "    # Instead of a grid-stride loop, each thread performs exactly pts_per_thread iterations.\n",
    "    for j in range(pts_per_thread):\n",
    "        x = xoroshiro128p_uniform_float32(states, i)\n",
    "        y = xoroshiro128p_uniform_float32(states, i)\n",
    "        if x*x + y*y <= 1.0:\n",
    "            local_count += 1\n",
    "\n",
    "    # Store the local result in shared memory.\n",
    "    shared[tid] = local_count\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    # Block-level reduction in shared memory.\n",
    "    stride = cuda.blockDim.x // 2\n",
    "    while stride > 0:\n",
    "        if tid < stride:\n",
    "            shared[tid] += shared[tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "    # The first thread in the block adds the block's total to global_count.\n",
    "    if tid == 0:\n",
    "        cuda.atomic.add(count, 0, shared[0])\n",
    "\n",
    "def estimate_pi_cuda_opt(nb_points):\n",
    "    threads_per_block = 256\n",
    "    total_threads = 2**15  # Total number of threads (and RNG states) to create.\n",
    "    blocks = total_threads // threads_per_block  # e.g., 1024/256 = 4 blocks\n",
    "    \n",
    "    # Ensure nb_points divides evenly by total_threads.\n",
    "    pts_per_thread = nb_points // total_threads\n",
    "\n",
    "    # Allocate a one-element array on the device for the global count.\n",
    "    count = np.array([0], dtype=np.int32)\n",
    "    d_count = cuda.to_device(count)\n",
    "\n",
    "    # Create a random state per thread.\n",
    "    states = create_xoroshiro128p_states(total_threads, seed=1)\n",
    "\n",
    "    print(\"total_threads:\", total_threads)\n",
    "    print(\"threads_per_block:\", threads_per_block)\n",
    "    print(\"pts_per_thread:\", pts_per_thread)\n",
    "    print(\"blocks:\", blocks)\n",
    "\n",
    "    # Launch the kernel.\n",
    "    estimate_pi_kernel_fixed[blocks, threads_per_block](states, pts_per_thread, d_count)\n",
    "\n",
    "    # Copy the result back to host and compute pi.\n",
    "    d_count.copy_to_host(count)\n",
    "    return 4.0 * count[0] / nb_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e357a-7ba7-4501-a4c9-d1e3a7f2636d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:26.385416Z",
     "iopub.status.busy": "2026-02-05T10:54:26.385129Z",
     "iopub.status.idle": "2026-02-05T10:54:26.820194Z",
     "shell.execute_reply": "2026-02-05T10:54:26.819658Z",
     "shell.execute_reply.started": "2026-02-05T10:54:26.385390Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_points = 2**31  # 16,777,216 total points\n",
    "print(\"original\")\n",
    "%time pi_estimate = estimate_pi_cuda(nb_points)\n",
    "print(pi_estimate)\n",
    "print('\\n***\\n')\n",
    "print(\"shared mem\")\n",
    "%time pi_estimate = estimate_pi_cuda_opt(nb_points)\n",
    "print(pi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338f493-acdf-4347-a05a-73dea473fc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

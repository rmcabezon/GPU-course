{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SICxTMkZg4Eh"
   },
   "source": [
    "# What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47qfI-M8L46r",
    "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU:\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "model name\t: Intel(R) Core(TM) i5-8600K CPU @ 3.60GHz\n",
      "GPU:\n",
      "Mon Jan 31 13:39:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   50C    P2    64W / 250W |    836MiB / 11175MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       937      G   /usr/lib/Xorg                     245MiB |\n",
      "|    0   N/A  N/A       990      G   /usr/bin/gnome-shell               90MiB |\n",
      "|    0   N/A  N/A      1428      G   ...AAAAAAAAA= --shared-files       35MiB |\n",
      "|    0   N/A  N/A      1563      G   ...AAAAAAAAA= --shared-files      115MiB |\n",
      "|    0   N/A  N/A      8364      G   /opt/zoom/zoom                     19MiB |\n",
      "|    0   N/A  N/A     13559      C   /usr/bin/python3                  169MiB |\n",
      "|    0   N/A  N/A     14844      C   /usr/bin/python3                  153MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSb5bpl6g-Iz"
   },
   "source": [
    "# Vector Addition\n",
    "\n",
    "We start by loading a few packages and we define some helper functions to generate the three vectors a, b and c, to compute the checksum of the result and to time the calculation.\n",
    "\n",
    "The standard Python implementation of the vector addition is provided for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "j-2GrrEMNPLa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numba import jit,njit,prange,cuda, types, float32\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Randomize between -10, 10\n",
    "def randomize_array(size):\n",
    "    return 10.0 * 2.0 * (rand(size) - 0.5)\n",
    "\n",
    "def init(size):\n",
    "    seed(2)\n",
    "    a = np.array(randomize_array(size), dtype=np.float32)\n",
    "    b = np.array(randomize_array(size), dtype=np.float32)\n",
    "    c = np.zeros(size, dtype=np.float32)\n",
    "    return a, b, c\n",
    "\n",
    "@njit(parallel = True)\n",
    "def check(c):\n",
    "    size = len(c)\n",
    "    sum = 0.0\n",
    "    for i in prange(size):\n",
    "        sum += c[i]\n",
    "    return sum\n",
    "\n",
    "def time_and_check(vec_op, size):\n",
    "    a, b, c = init(size)\n",
    "\n",
    "    start = time.time()\n",
    "    vec_op(a, b, c)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
    "\n",
    "# Python implementation\n",
    "@njit(parallel = True)\n",
    "def vec_add_interpreted(a, b, c):\n",
    "    for i in range(len(a)):\n",
    "        c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjyY1tO_h8hp"
   },
   "source": [
    "The addition of two vectors is very straightforward. The complexity is linear with the size of the input. Therefore here we use a large vector size to increase the execution time. Note that we use a power of two, as this will help us a bit with the CUDA implementation at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPLBww188o61",
    "outputId": "e486a3b4-dff5-49bc-ecf4-9e5f99f01943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted Python:\n",
      "Size:  4194304  elapsed time:  0.8117320537567139  checksum =  -204.1574936332181\n",
      "Size:  4194304  elapsed time:  0.8539071083068848  checksum =  -204.1574936332181\n"
     ]
    }
   ],
   "source": [
    "size = 2**26\n",
    "\n",
    "print(\"Interpreted Python:\")\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_interpreted, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwG7GcJuh1rc"
   },
   "source": [
    "# The CUDA implementation\n",
    "\n",
    "Now it's your turn to implement the CUDA kernel! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-acKLc1ksNt"
   },
   "source": [
    "## Exercise 1: The CUDA Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6XE-RxFQRQ29"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (2348378693.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [5]\u001b[0;36m\u001b[0m\n\u001b[0;31m    blocksize = # 3. block size = number of threads per block dimension\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    # get thread position 'i' in the grid\n",
    "    # do computation at grid position 'i'\n",
    "\n",
    "# call the function\n",
    "\n",
    "size = 2**26\n",
    "\n",
    "blocksize = # block size = number of threads per block dimension\n",
    "gridsize = # grid size = number of blocks per grid dimension\n",
    "\n",
    "# Check!\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuQPO1Bqllwk"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iB4GEsGf8o61"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    i = cuda.grid(1)\n",
    "    c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeM9nVCWw6AM",
    "outputId": "03a6c781-37db-437e-d9fb-b704f813d8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  4194304  elapsed time:  0.10367012023925781  checksum =  -204.1574936332181\n",
      "Size:  4194304  elapsed time:  0.010380268096923828  checksum =  -204.1574936332181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acavelan/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "size = 2**26\n",
    "\n",
    "blocksize = 32\n",
    "gridsize = int(size/blocksize)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynqVJukXjQFQ"
   },
   "source": [
    "## Exercise 2: Foolproof\n",
    "\n",
    "Adapt the previous code to handle sizes which are **not** a power of 2. **Hint:** you need to change both the kernel and the gridsize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "l1qJI66nk-vv",
    "outputId": "9269663e-d4a9-45e0-bf58-163bbc83f307"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (3633157771.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [8]\u001b[0;36m\u001b[0m\n\u001b[0;31m    blocksize =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    # check that thread position 'i' is valid\n",
    "\n",
    "blocksize =\n",
    "gridsize = \n",
    "\n",
    "size = 12345678\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCk9zm33ln4H"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cQTW-GUIjaF_"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i < a.shape[0]:\n",
    "        c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f17T8PiHwrmL",
    "outputId": "e0636008-9c36-48c6-a758-6b42fb6da04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  12345678  elapsed time:  2.752197027206421  checksum =  9072.72968535521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acavelan/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  12345678  elapsed time:  0.10360145568847656  checksum =  9072.72968535521\n",
      "Size:  12345678  elapsed time:  0.06505155563354492  checksum =  9072.72968535521\n"
     ]
    }
   ],
   "source": [
    "size = 12345678\n",
    "\n",
    "blocksize = 32\n",
    "gridsize = int((size+blocksize)/blocksize)\n",
    "\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibfklGMzlxgI"
   },
   "source": [
    "## Exercise 3: Simple Memory Management\n",
    "\n",
    "By default, if we let Numba take care of the data transfers, Numba will copy all three arrays to and from the device everytime. \n",
    "\n",
    "This would be a good time to do some profiling using nvprof:\n",
    "```\n",
    "==8912== Profiling application: python vec_add.py\n",
    "==8912== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:   61.98%  79.743ms         6  13.290ms  5.6272ms  28.232ms  [CUDA memcpy DtoH]\n",
    "                   36.66%  47.159ms         6  7.8599ms  5.3962ms  12.592ms  [CUDA memcpy HtoD]\n",
    "                    1.36%  1.7535ms         2  876.77us  876.74us  876.80us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
    "      API calls:   39.09%  83.395ms         6  13.899ms  5.7020ms  29.060ms  cuMemcpyDtoH\n",
    "                   38.00%  81.068ms         1  81.068ms  81.068ms  81.068ms  cuDevicePrimaryCtxRetain\n",
    "                   22.22%  47.419ms         6  7.9031ms  5.3943ms  12.724ms  cuMemcpyHtoD\n",
    "```\n",
    "\n",
    "**61.98**% of the total time is spent in the **[CUDA memcpy DtoH]** function, and **[CUDA memcpy HtoD]** function. **In total, 98.6% of the total execution time on the GPU is lost in data transfers...** Yes, only 1.36% of time is calculations.\n",
    "\n",
    "But in fact, we don't need to copy all three arrays everytime. We need to copy array a and b **to** the device (c will be set on the device), and we need to copy array c **from** the device to get the results.\n",
    "\n",
    "Below are some examples of how to control data transfers manually:\n",
    "\n",
    "```\n",
    "# Create device array d_a from array a and copy it to the device\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Alternatively, create device array d_c from array c but DON'T copy it\n",
    "d_c = cuda.to_device(c, copy=False)\n",
    "\n",
    "# Copy the content of device array d_c to host array c\n",
    "d_c.copy_to_host(c)\n",
    "``` \n",
    "\n",
    "Then when calling the kernel function, use the freshly created device arrays rather than the host arrays:\n",
    "```\n",
    "vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "12kgWZufqxI1"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (2844661861.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    size = 2**24\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "# The CUDA kernel remains the same\n",
    "# We only change the launch\n",
    "def vec_add_numba_cuda_no_copy(a, b, c):\n",
    "    size = 2**26\n",
    "\n",
    "    blocksize = \n",
    "    gridsize = \n",
    "    \n",
    "    # Launch the CUDA kernel vec_add_numba_cuda here\n",
    "\n",
    "print(\"Cuda:\")\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "print(\"Cuda no copy:\")\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB6P8qdittRC"
   },
   "source": [
    "\n",
    "If you did it right, it should be significantly faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6nmFc_iqvno"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CGT1rjZA8o62"
   },
   "outputs": [],
   "source": [
    "def vec_add_numba_cuda_no_copy(a, b, c):\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_c = cuda.to_device(c, copy=False)\n",
    "\n",
    "    blocksize = 32\n",
    "    gridsize = int(size / blocksize)\n",
    "\n",
    "    vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
    "\n",
    "    d_c.copy_to_host(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M482Um5qxzI3",
    "outputId": "005fdc63-1ac2-4579-aa1e-f9ac3152389c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda:\n",
      "Size:  67108864  elapsed time:  0.2748384475708008  checksum =  92282.16897344025\n",
      "Size:  67108864  elapsed time:  0.2865443229675293  checksum =  92282.16897344025\n",
      "Cuda no copy:\n",
      "Size:  67108864  elapsed time:  0.14850401878356934  checksum =  92282.16897344025\n",
      "Size:  67108864  elapsed time:  0.14885616302490234  checksum =  92282.16897344025\n"
     ]
    }
   ],
   "source": [
    "size = 2**26\n",
    "print(\"Cuda:\")\n",
    "blocksize = 32\n",
    "gridsize = int(size / blocksize)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "print(\"Cuda no copy:\")\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzEHZwXusmBD"
   },
   "source": [
    "```\n",
    "==9108== Profiling application: python vec_add.py\n",
    "==9108== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:   67.61%  49.289ms         2  24.644ms  24.631ms  24.658ms  [CUDA memcpy DtoH]\n",
    "                   29.98%  21.857ms         4  5.4642ms  5.4225ms  5.5229ms  [CUDA memcpy HtoD]\n",
    "                    2.41%  1.7536ms         2  876.79us  876.16us  877.41us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
    "      API calls:   50.14%  76.012ms         1  76.012ms  76.012ms  76.012ms  cuDevicePrimaryCtxRetain\n",
    "                   34.52%  52.337ms         2  26.168ms  26.138ms  26.199ms  cuMemcpyDtoH\n",
    "                   14.41%  21.845ms         4  5.4613ms  5.4113ms  5.4877ms  cuMemcpyHtoD\n",
    "```\n",
    "\n",
    "We have doubled the time spent on calculations! The whole execution is a factor 2x faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQkWxRqtHND"
   },
   "source": [
    "# Exercise 4: Tiling (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGkBa2F2tyFI"
   },
   "source": [
    "Cuda **streams** allow concurrency of execution on a single device within a given context. Queued work items in the same stream execute sequentially, but work items in different streams may execute concurrently. Most operations involving a CUDA device can be performed asynchronously using streams, including data transfers and kernel execution.\n",
    "\n",
    "In this example, computation time is too small to get any benefit, however we can try overlapping data transfers.\n",
    "\n",
    "This is done with a technique called **Tiling**, where we split the work into smaller work items (also called chunks).\n",
    "\n",
    "Here the code is given as an example, and you will not see any benefit in google colab. On a newer GPU, this will be 20 to 40% faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EV_QrPMW8o62",
    "outputId": "465af85d-f365-412e-d090-5da8e3082385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda:\n",
      "Size:  16777216  elapsed time:  0.10087394714355469  checksum =  43100.967578660464\n",
      "Size:  16777216  elapsed time:  0.06922435760498047  checksum =  43100.967578660464\n",
      "Cuda no copy:\n",
      "Size:  16777216  elapsed time:  0.038695335388183594  checksum =  43100.967578660464\n",
      "Size:  16777216  elapsed time:  0.038260459899902344  checksum =  43100.967578660464\n",
      "Cuda tiling:\n",
      "Size:  16777216  elapsed time:  0.07054519653320312  checksum =  43100.967578660464\n",
      "Size:  16777216  elapsed time:  0.05433249473571777  checksum =  43100.967578660464\n"
     ]
    }
   ],
   "source": [
    "def vec_add_numba_cuda_tiling(a, b, c):\n",
    "    nChunks = 8\n",
    "    chunkSize = int(size / nChunks)\n",
    "\n",
    "    for i in range(nChunks):\n",
    "        stream = cuda.stream()\n",
    "        \n",
    "        begin = i * chunkSize\n",
    "        end = begin + chunkSize\n",
    "\n",
    "        d_a = cuda.to_device(a[begin:end], stream=stream)\n",
    "        d_b = cuda.to_device(b[begin:end], stream=stream)\n",
    "        d_c = cuda.to_device(c[begin:end], stream=stream, copy=False)\n",
    "\n",
    "        blocksize = 32\n",
    "        gridsize = int(size / blocksize)\n",
    "\n",
    "        #print(\"ChunkSize = \", chunkSize, \" tpb = \", TPB, \" gridsize = \", gridsize)\n",
    "        vec_add_numba_cuda[gridsize, blocksize, stream](d_a, d_b, d_c)\n",
    "\n",
    "        d_c.copy_to_host(c[begin:end], stream=stream)\n",
    "        \n",
    "    \n",
    "size = 2**24\n",
    "print(\"Cuda:\")\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "print(\"Cuda no copy:\")\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
    "print(\"Cuda tiling:\")\n",
    "time_and_check(vec_add_numba_cuda_tiling, size)\n",
    "time_and_check(vec_add_numba_cuda_tiling, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vec_add_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:mc3py11-hf]",
   "language": "python",
   "name": "conda-env-mc3py11-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What CPU and GPU am I using?\n",
        "\n",
        "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
      ],
      "metadata": {
        "id": "SICxTMkZg4Eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47qfI-M8L46r",
        "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU:\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "GPU:\n",
            "Fri Jan 28 09:32:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!echo \"CPU:\"\n",
        "!cat /proc/cpuinfo | grep name\n",
        "!echo \"GPU:\"\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Addition\n",
        "\n",
        "We start by loading a few packages and we define some helper functions to generate the three vectors a, b and c, to compute the checksum of the result and to time the calculation.\n",
        "\n",
        "The standard Python implementation of the vector addition is provided for reference."
      ],
      "metadata": {
        "id": "pSb5bpl6g-Iz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-2GrrEMNPLa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "from numba import jit,njit,prange,cuda, types, float32\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Randomize between -10, 10\n",
        "def randomize_array(size):\n",
        "    return 10.0 * 2.0 * (rand(size) - 0.5)\n",
        "\n",
        "def init(size):\n",
        "    seed(2)\n",
        "    a = np.array(randomize_array(size), dtype=np.float32)\n",
        "    b = np.array(randomize_array(size), dtype=np.float32)\n",
        "    c = np.zeros(size, dtype=np.float32)\n",
        "    return a, b, c\n",
        "\n",
        "@njit(parallel = True)\n",
        "def check(c):\n",
        "    size = len(c)\n",
        "    sum = 0.0\n",
        "    for i in prange(size):\n",
        "        sum += c[i]\n",
        "    return sum\n",
        "\n",
        "def time_and_check(vec_op, size):\n",
        "    a, b, c = init(size)\n",
        "\n",
        "    start = time.time()\n",
        "    vec_op(a, b, c)\n",
        "    end = time.time()\n",
        "\n",
        "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
        "\n",
        "# Python implementation\n",
        "def vec_add_interpreted(a, b, c):\n",
        "    for i in range(len(a)):\n",
        "        c[i] = a[i] + b[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The addition of two vectors is very straightforward. The complexity is linear with the size of the input. Therefore here we use a large vector size to increase the execution time. Note that we use a power of two, as this will help us a bit with the CUDA implementation at first."
      ],
      "metadata": {
        "id": "hjyY1tO_h8hp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLBww188o61",
        "outputId": "e486a3b4-dff5-49bc-ecf4-9e5f99f01943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpreted Python:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  4194304  elapsed time:  1.9601237773895264  checksum =  -204.1574936332181\n",
            "Size:  4194304  elapsed time:  1.9934566020965576  checksum =  -204.1574936332181\n"
          ]
        }
      ],
      "source": [
        "size = 2**22\n",
        "\n",
        "print(\"Interpreted Python:\")\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_interpreted, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA implementation\n",
        "\n",
        "This is a very simple operation, therefore, you will have to implement the CUDA kernel function by yourself. "
      ],
      "metadata": {
        "id": "UwG7GcJuh1rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: The CUDA Hello World"
      ],
      "metadata": {
        "id": "h-acKLc1ksNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    # get thread position 'i' in the grid\n",
        "    # do computation at grid position 'i'\n",
        "\n",
        "# call the function\n",
        "\n",
        "blocksize = # 3. block size = number of threads per block dimension\n",
        "gridsize = # 4. grid size = number of blocks per grid dimension\n",
        "\n",
        "# Check!\n",
        "size = 2**20\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "id": "6XE-RxFQRQ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "JuQPO1Bqllwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB4GEsGf8o61"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    i = cuda.grid(1)\n",
        "    c[i] = a[i] + b[i]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 2**22\n",
        "\n",
        "blocksize = 32\n",
        "gridsize = int(size/blocksize)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeM9nVCWw6AM",
        "outputId": "03a6c781-37db-437e-d9fb-b704f813d8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  4194304  elapsed time:  0.5735862255096436  checksum =  -204.1574936332181\n",
            "Size:  4194304  elapsed time:  0.01816868782043457  checksum =  -204.1574936332181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Foolproof\n",
        "\n",
        "Adapt the previous code to handle sizes which are not a power of 2. Hint: you need to change both the kernel and the gridsize.\n"
      ],
      "metadata": {
        "id": "ynqVJukXjQFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_add_numba_cuda(a, b, c):\n",
        "    # check that thread position 'i' is valid\n",
        "\n",
        "blocksize =\n",
        "gridsize = \n",
        "\n",
        "size = 12345678\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "l1qJI66nk-vv",
        "outputId": "9269663e-d4a9-45e0-bf58-163bbc83f307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-8383f77bbcca>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    blocksize =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "ZCk9zm33ln4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    i = cuda.grid(1)\n",
        "\n",
        "    if i < a.shape[0]:\n",
        "        c[i] = a[i] + b[i]"
      ],
      "metadata": {
        "id": "cQTW-GUIjaF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 12345678\n",
        "\n",
        "blocksize = 32\n",
        "gridsize = int((size+blocksize)/blocksize)\n",
        "\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f17T8PiHwrmL",
        "outputId": "e0636008-9c36-48c6-a758-6b42fb6da04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  12345678  elapsed time:  5.597100496292114  checksum =  9072.72968535521\n",
            "Size:  12345678  elapsed time:  0.2529134750366211  checksum =  9072.72968535521\n",
            "Size:  12345678  elapsed time:  0.08398103713989258  checksum =  9072.72968535521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Memory Management\n",
        "\n",
        "By default, if we let Numba take care of the data transfers, Numba will copy all three arrays to and from the device everytime. \n",
        "\n",
        "This would be a good time to do some profiling using nvprof:\n",
        "```\n",
        "==8912== Profiling application: python vec_add.py\n",
        "==8912== Profiling result:\n",
        "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
        " GPU activities:   61.98%  79.743ms         6  13.290ms  5.6272ms  28.232ms  [CUDA memcpy DtoH]\n",
        "                   36.66%  47.159ms         6  7.8599ms  5.3962ms  12.592ms  [CUDA memcpy HtoD]\n",
        "                    1.36%  1.7535ms         2  876.77us  876.74us  876.80us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
        "      API calls:   39.09%  83.395ms         6  13.899ms  5.7020ms  29.060ms  cuMemcpyDtoH\n",
        "                   38.00%  81.068ms         1  81.068ms  81.068ms  81.068ms  cuDevicePrimaryCtxRetain\n",
        "                   22.22%  47.419ms         6  7.9031ms  5.3943ms  12.724ms  cuMemcpyHtoD\n",
        "```\n",
        "\n",
        "**61.98**% of the total time is spent in the **[CUDA memcpy DtoH]** function, and **[CUDA memcpy HtoD]** function. **In total, 98.6% of the total execution time on the GPU is lost in data transfers...** Yes, only 1.36% of time is calculations.\n",
        "\n",
        "But in fact, we don't need to copy all three arrays everytime. We need to copy array a and b **to** the device (c will be set on the device), and we need to copy array c **from** the device to get the results.\n",
        "\n",
        "Below are some examples of how to control data transfers manually:\n",
        "\n",
        "```\n",
        "# Create device array d_a from array a and copy it to the device\n",
        "d_a = cuda.to_device(a)\n",
        "\n",
        "# Alternatively, create device array d_c from array c but DON'T copy it\n",
        "d_c = cuda.to_device(c, copy=False)\n",
        "\n",
        "# Copy the content of device array d_c to host array c\n",
        "d_c.copy_to_host(c)\n",
        "``` \n",
        "\n",
        "Then when calling the kernel function, use the freshly created device arrays rather than the host arrays:\n",
        "```\n",
        "vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
        "```"
      ],
      "metadata": {
        "id": "ibfklGMzlxgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_add_numba_cuda_no_copy(a, b, c):\n",
        "    # your turn\n",
        "        \n",
        "    \n",
        "size = 2**24\n",
        "\n",
        "blocksize = \n",
        "gridsize = \n",
        "\n",
        "print(\"Cuda:\")\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)"
      ],
      "metadata": {
        "id": "12kgWZufqxI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you did it right, it should be significantly faster!"
      ],
      "metadata": {
        "id": "sB6P8qdittRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "W6nmFc_iqvno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGT1rjZA8o62"
      },
      "outputs": [],
      "source": [
        "def vec_add_numba_cuda_no_copy(a, b, c):\n",
        "    d_a = cuda.to_device(a)\n",
        "    d_b = cuda.to_device(b)\n",
        "    d_c = cuda.to_device(c, copy=False)\n",
        "\n",
        "    blocksize = 32\n",
        "    gridsize = int(size / blocksize)\n",
        "\n",
        "    vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
        "\n",
        "    d_c.copy_to_host(c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 2**22\n",
        "print(\"Cuda:\")\n",
        "blocksize = 32\n",
        "gridsize = int(size / blocksize)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M482Um5qxzI3",
        "outputId": "005fdc63-1ac2-4579-aa1e-f9ac3152389c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda:\n",
            "Size:  134217728  elapsed time:  0.6065824031829834  checksum =  6869.8527526276885\n",
            "Size:  134217728  elapsed time:  0.5396871566772461  checksum =  6869.8527526276885\n",
            "Cuda no copy:\n",
            "Size:  134217728  elapsed time:  0.1997087001800537  checksum =  6869.8527526276885\n",
            "Size:  134217728  elapsed time:  0.382235050201416  checksum =  6869.8527526276885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "==9108== Profiling application: python vec_add.py\n",
        "==9108== Profiling result:\n",
        "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
        " GPU activities:   67.61%  49.289ms         2  24.644ms  24.631ms  24.658ms  [CUDA memcpy DtoH]\n",
        "                   29.98%  21.857ms         4  5.4642ms  5.4225ms  5.5229ms  [CUDA memcpy HtoD]\n",
        "                    2.41%  1.7536ms         2  876.79us  876.16us  877.41us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
        "      API calls:   50.14%  76.012ms         1  76.012ms  76.012ms  76.012ms  cuDevicePrimaryCtxRetain\n",
        "                   34.52%  52.337ms         2  26.168ms  26.138ms  26.199ms  cuMemcpyDtoH\n",
        "                   14.41%  21.845ms         4  5.4613ms  5.4113ms  5.4877ms  cuMemcpyHtoD\n",
        "```\n",
        "\n",
        "We have doubled the time spent on calculations! The whole execution is a factor 2x faster!"
      ],
      "metadata": {
        "id": "bzEHZwXusmBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Tiling"
      ],
      "metadata": {
        "id": "qRQkWxRqtHND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuda **streams** allow concurrency of execution on a single device within a given context. Queued work items in the same stream execute sequentially, but work items in different streams may execute concurrently. Most operations involving a CUDA device can be performed asynchronously using streams, including data transfers and kernel execution.\n",
        "\n",
        "In this example, computation time is too small to get any benefit, however we can try overlapping data transfers.\n",
        "\n",
        "This is done with a technique called **Tiling**, where we split the work into smaller work items (also called chunks).\n",
        "\n",
        "Here the code is given as an example, and you will not see any benefit in google colab. On a newer GPU, this will be 20 to 40% faster."
      ],
      "metadata": {
        "id": "aGkBa2F2tyFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV_QrPMW8o62",
        "outputId": "465af85d-f365-412e-d090-5da8e3082385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda:\n",
            "Size:  16777216  elapsed time:  0.05959486961364746  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.057906150817871094  checksum =  43100.967578660464\n",
            "Cuda no copy:\n",
            "Size:  16777216  elapsed time:  0.029003381729125977  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.12186980247497559  checksum =  43100.967578660464\n",
            "Cuda tiling:\n",
            "Size:  16777216  elapsed time:  0.08003592491149902  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.06699371337890625  checksum =  43100.967578660464\n"
          ]
        }
      ],
      "source": [
        "def vec_add_numba_cuda_tiling(a, b, c):\n",
        "    nChunks = 8\n",
        "    chunkSize = int(size / nChunks)\n",
        "\n",
        "    for i in range(nChunks):\n",
        "        stream = cuda.stream()\n",
        "        \n",
        "        begin = i * chunkSize\n",
        "        end = begin + chunkSize\n",
        "\n",
        "        d_a = cuda.to_device(a[begin:end], stream=stream)\n",
        "        d_b = cuda.to_device(b[begin:end], stream=stream)\n",
        "        d_c = cuda.to_device(c[begin:end], stream=stream, copy=False)\n",
        "\n",
        "        blocksize = 32\n",
        "        gridsize = int(size / blocksize)\n",
        "\n",
        "        #print(\"ChunkSize = \", chunkSize, \" tpb = \", TPB, \" gridsize = \", gridsize)\n",
        "        vec_add_numba_cuda[gridsize, blocksize, stream](d_a, d_b, d_c)\n",
        "\n",
        "        d_c.copy_to_host(c[begin:end], stream=stream)\n",
        "        \n",
        "    \n",
        "size = 2**24\n",
        "print(\"Cuda:\")\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "print(\"Cuda tiling:\")\n",
        "time_and_check(vec_add_numba_cuda_tiling, size)\n",
        "time_and_check(vec_add_numba_cuda_tiling, size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "vec_add_exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SICxTMkZg4Eh"
   },
   "source": [
    "# What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47qfI-M8L46r",
    "outputId": "e2eada9b-8d00-4d77-dadf-dfe92fff2b9f"
   },
   "outputs": [],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSb5bpl6g-Iz"
   },
   "source": [
    "# Vector Addition\n",
    "\n",
    "We start by loading a few packages and we define some helper functions to generate the three vectors a, b and c, to compute the checksum of the result and to time the calculation.\n",
    "\n",
    "The standard Python implementation of the vector addition is provided for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-2GrrEMNPLa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numba import jit,njit,prange,cuda, types, float32\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Randomize between -10, 10\n",
    "def randomize_array(size):\n",
    "    return 10.0 * 2.0 * (rand(size) - 0.5)\n",
    "\n",
    "def init(size):\n",
    "    seed(2)\n",
    "    a = np.array(randomize_array(size), dtype=np.float32)\n",
    "    b = np.array(randomize_array(size), dtype=np.float32)\n",
    "    c = np.zeros(size, dtype=np.float32)\n",
    "    return a, b, c\n",
    "\n",
    "@njit(parallel = True)\n",
    "def check(c):\n",
    "    size = len(c)\n",
    "    sum = 0.0\n",
    "    for i in prange(size):\n",
    "        sum += c[i]\n",
    "    return sum\n",
    "\n",
    "def time_and_check(vec_op, size):\n",
    "    a, b, c = init(size)\n",
    "\n",
    "    start = time.time()\n",
    "    vec_op(a, b, c)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
    "\n",
    "# Python implementation\n",
    "@njit(parallel = True)\n",
    "def vec_add_interpreted(a, b, c):\n",
    "    for i in prange(len(a)):\n",
    "        c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjyY1tO_h8hp"
   },
   "source": [
    "The addition of two vectors is very straightforward. The complexity is linear with the size of the input. Therefore here we use a large vector size to increase the execution time. Note that we use a power of two, as this will help us a bit with the CUDA implementation at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPLBww188o61",
    "outputId": "e486a3b4-dff5-49bc-ecf4-9e5f99f01943"
   },
   "outputs": [],
   "source": [
    "size = 2**26\n",
    "\n",
    "print(\"Interpreted Python:\")\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_interpreted, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwG7GcJuh1rc",
    "tags": []
   },
   "source": [
    "# The CUDA implementation\n",
    "\n",
    "Now it's your turn to implement the CUDA kernel! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-acKLc1ksNt"
   },
   "source": [
    "## Exercise 1: The CUDA Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XE-RxFQRQ29"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    # get thread position 'i' in the grid\n",
    "    # do computation at grid position 'i'\n",
    "\n",
    "# call the function\n",
    "\n",
    "size = 2**26\n",
    "\n",
    "blocksize = # block size = number of threads per block dimension\n",
    "gridsize = # grid size = number of blocks per grid dimension\n",
    "\n",
    "# Check!\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynqVJukXjQFQ",
    "tags": []
   },
   "source": [
    "## Exercise 2: Foolproof\n",
    "\n",
    "Adapt the previous code to handle sizes which are **not** a power of 2. **Hint:** you need to change both the kernel and the gridsize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "l1qJI66nk-vv",
    "outputId": "9269663e-d4a9-45e0-bf58-163bbc83f307"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vec_add_numba_cuda(a, b, c):\n",
    "    # check that thread position 'i' is valid\n",
    "    # hint: you can get the length of array a with a.shape[0]\n",
    "\n",
    "size = 12345678\n",
    "\n",
    "blocksize =\n",
    "gridsize = \n",
    "\n",
    "time_and_check(vec_add_interpreted, size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibfklGMzlxgI",
    "tags": []
   },
   "source": [
    "## Exercise 3: Simple Memory Management\n",
    "\n",
    "By default, if we let Numba take care of the data transfers, Numba will copy all three arrays to and from the device everytime. \n",
    "\n",
    "This would be a good time to do some profiling using nvprof:\n",
    "```\n",
    "==8912== Profiling application: python vec_add.py\n",
    "==8912== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:   61.98%  79.743ms         6  13.290ms  5.6272ms  28.232ms  [CUDA memcpy DtoH]\n",
    "                   36.66%  47.159ms         6  7.8599ms  5.3962ms  12.592ms  [CUDA memcpy HtoD]\n",
    "                    1.36%  1.7535ms         2  876.77us  876.74us  876.80us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
    "      API calls:   39.09%  83.395ms         6  13.899ms  5.7020ms  29.060ms  cuMemcpyDtoH\n",
    "                   38.00%  81.068ms         1  81.068ms  81.068ms  81.068ms  cuDevicePrimaryCtxRetain\n",
    "                   22.22%  47.419ms         6  7.9031ms  5.3943ms  12.724ms  cuMemcpyHtoD\n",
    "```\n",
    "\n",
    "**61.98**% of the total time is spent in the **[CUDA memcpy DtoH]** function, and **[CUDA memcpy HtoD]** function. **In total, 98.6% of the total execution time on the GPU is lost in data transfers...** Yes, only 1.36% of time is calculations.\n",
    "\n",
    "But in fact, we don't need to copy all three arrays everytime. We need to copy array a and b **to** the device (c will be set on the device), and we need to copy array c **from** the device to get the results.\n",
    "\n",
    "Below are some examples of how to control data transfers manually:\n",
    "\n",
    "```\n",
    "# Create device array d_a from array a and copy it to the device\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Alternatively, create device array d_c from array c but DON'T copy it\n",
    "d_c = cuda.to_device(c, copy=False)\n",
    "\n",
    "# Copy the content of device array d_c to host array c\n",
    "d_c.copy_to_host(c)\n",
    "``` \n",
    "\n",
    "Then when calling the kernel function, use the freshly created device arrays rather than the host arrays:\n",
    "```\n",
    "vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12kgWZufqxI1"
   },
   "outputs": [],
   "source": [
    "# The CUDA kernel remains the same\n",
    "# We only change the launch\n",
    "def vec_add_numba_cuda_no_copy(a, b, c):\n",
    "    size = 2**26\n",
    "\n",
    "    blocksize = \n",
    "    gridsize = \n",
    "    \n",
    "    # Launch the CUDA kernel vec_add_numba_cuda here\n",
    "\n",
    "print(\"Cuda:\")\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
    "print(\"Cuda no copy:\")\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
    "time_and_check(vec_add_numba_cuda_no_copy, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB6P8qdittRC"
   },
   "source": [
    "\n",
    "If you did it right, it should be significantly faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzEHZwXusmBD"
   },
   "source": [
    "```\n",
    "==9108== Profiling application: python vec_add.py\n",
    "==9108== Profiling result:\n",
    "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    " GPU activities:   67.61%  49.289ms         2  24.644ms  24.631ms  24.658ms  [CUDA memcpy DtoH]\n",
    "                   29.98%  21.857ms         4  5.4642ms  5.4225ms  5.5229ms  [CUDA memcpy HtoD]\n",
    "                    2.41%  1.7536ms         2  876.79us  876.16us  877.41us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
    "      API calls:   50.14%  76.012ms         1  76.012ms  76.012ms  76.012ms  cuDevicePrimaryCtxRetain\n",
    "                   34.52%  52.337ms         2  26.168ms  26.138ms  26.199ms  cuMemcpyDtoH\n",
    "                   14.41%  21.845ms         4  5.4613ms  5.4113ms  5.4877ms  cuMemcpyHtoD\n",
    "```\n",
    "\n",
    "We have doubled the time spent on calculations! The whole execution is a factor 2x faster!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vec_add_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gpu-course)",
   "language": "python",
   "name": "gpu-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

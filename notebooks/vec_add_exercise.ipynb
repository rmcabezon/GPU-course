{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What CPU and GPU am I using?\n",
        "\n",
        "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
      ],
      "metadata": {
        "id": "SICxTMkZg4Eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47qfI-M8L46r",
        "outputId": "11f98d90-fc1b-4c8a-d0c0-f742efc6410d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU:\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "GPU:\n",
            "Tue Jan 25 12:53:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!echo \"CPU:\"\n",
        "!cat /proc/cpuinfo | grep name\n",
        "!echo \"GPU:\"\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Addition\n",
        "\n",
        "We start by loading a few packages and we define some helper functions to generate the three vectors a, b and c, to compute the checksum of the result and to time the calculation.\n",
        "\n",
        "The standard Python implementation of the vector addition is provided for reference."
      ],
      "metadata": {
        "id": "pSb5bpl6g-Iz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j-2GrrEMNPLa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "from numba import jit,njit,prange,cuda, types, float32\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Randomize between -10, 10\n",
        "def randomize_array(size):\n",
        "    return 10.0 * 2.0 * (rand(size) - 0.5)\n",
        "\n",
        "def init(size):\n",
        "    seed(2)\n",
        "    a = np.array(randomize_array(size), dtype=np.float32)\n",
        "    b = np.array(randomize_array(size), dtype=np.float32)\n",
        "    c = np.zeros(size, dtype=np.float32)\n",
        "    return a, b, c\n",
        "\n",
        "@njit(parallel = True)\n",
        "def check(c):\n",
        "    size = len(c)\n",
        "    sum = 0.0\n",
        "    for i in prange(size):\n",
        "        sum += c[i]\n",
        "    return sum\n",
        "\n",
        "def time_and_check(vec_op, size):\n",
        "    a, b, c = init(size)\n",
        "\n",
        "    start = time.time()\n",
        "    vec_op(a, b, c)\n",
        "    end = time.time()\n",
        "\n",
        "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
        "\n",
        "# Python implementation\n",
        "def vec_add_interpreted(a, b, c):\n",
        "    for i in range(len(a)):\n",
        "        c[i] = a[i] + b[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The addition of two vectors is very straightforward. The complexity is linear with the size of the input. Therefore here we use a large vector size to increase the execution time. Note that we use a power of two, as this will help us a bit with the CUDA implementation at first."
      ],
      "metadata": {
        "id": "hjyY1tO_h8hp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLBww188o61",
        "outputId": "6c4e0962-d101-412a-caab-cd7753386e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpreted Python:\n",
            "Size:  4194304  elapsed time:  2.033522844314575  checksum =  -204.1574936332181\n",
            "Size:  4194304  elapsed time:  1.9677560329437256  checksum =  -204.1574936332181\n"
          ]
        }
      ],
      "source": [
        "size = 2**22\n",
        "\n",
        "print(\"Interpreted Python:\")\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_interpreted, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA implementation\n",
        "\n",
        "This is a very simple operation, therefore, you will have to implement the CUDA kernel function by yourself. "
      ],
      "metadata": {
        "id": "UwG7GcJuh1rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: The CUDA Hello World"
      ],
      "metadata": {
        "id": "h-acKLc1ksNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    # get thread position 'i' in the grid\n",
        "    # do computation at grid position 'i'\n",
        "\n",
        "# call the function\n",
        "\n",
        "blocksize = # 3. block size = number of threads per block dimension\n",
        "gridsize = # 4. grid size = number of blocks per grid dimension\n",
        "\n",
        "# Check!\n",
        "size = 2**20\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "id": "6XE-RxFQRQ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "JuQPO1Bqllwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iB4GEsGf8o61"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    i = cuda.grid(1)\n",
        "    c[i] = a[i] + b[i]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 2**22\n",
        "\n",
        "TPB = 32\n",
        "blocksize = TPB\n",
        "gridsize = int(size/TPB)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeM9nVCWw6AM",
        "outputId": "d3104b71-5b22-44ae-9bfd-ca854551e87e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  4194304  elapsed time:  0.0993962287902832  checksum =  -204.1574936332181\n",
            "Size:  4194304  elapsed time:  0.019332170486450195  checksum =  -204.1574936332181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Foolproof\n",
        "\n",
        "Adapt the previous code to handle sizes which are not a power of 2. Hint: you need to change both the kernel and the gridsize.\n"
      ],
      "metadata": {
        "id": "ynqVJukXjQFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_add_numba_cuda(a, b, c):\n",
        "    # check that thread position 'i' is valid\n",
        "\n",
        "blocksize =\n",
        "gridsize = \n",
        "\n",
        "size = 12345678\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "id": "l1qJI66nk-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "ZCk9zm33ln4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def vec_add_numba_cuda(a, b, c):\n",
        "    i = cuda.grid(1)\n",
        "\n",
        "    if i < a.shape[0]:\n",
        "        c[i] = a[i] + b[i]"
      ],
      "metadata": {
        "id": "cQTW-GUIjaF_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 12345678\n",
        "\n",
        "blocksize = 32\n",
        "gridsize = int((size+blocksize)/blocksize)\n",
        "\n",
        "time_and_check(vec_add_interpreted, size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f17T8PiHwrmL",
        "outputId": "eb084c51-9bc2-4a5f-daa0-85d2e361af24"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  12345678  elapsed time:  5.73702597618103  checksum =  9072.72968535521\n",
            "Size:  12345678  elapsed time:  0.05341076850891113  checksum =  9072.72968535521\n",
            "Size:  12345678  elapsed time:  0.0923762321472168  checksum =  9072.72968535521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Memory Management\n",
        "\n",
        "By default, if we let Numba take care of the data transfers, Numba will copy all three arrays to and from the device everytime. \n",
        "\n",
        "This would be a good time to do some profiling using nvprof:\n",
        "```\n",
        "==8912== Profiling application: python vec_add.py\n",
        "==8912== Profiling result:\n",
        "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
        " GPU activities:   61.98%  79.743ms         6  13.290ms  5.6272ms  28.232ms  [CUDA memcpy DtoH]\n",
        "                   36.66%  47.159ms         6  7.8599ms  5.3962ms  12.592ms  [CUDA memcpy HtoD]\n",
        "                    1.36%  1.7535ms         2  876.77us  876.74us  876.80us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
        "      API calls:   39.09%  83.395ms         6  13.899ms  5.7020ms  29.060ms  cuMemcpyDtoH\n",
        "                   38.00%  81.068ms         1  81.068ms  81.068ms  81.068ms  cuDevicePrimaryCtxRetain\n",
        "                   22.22%  47.419ms         6  7.9031ms  5.3943ms  12.724ms  cuMemcpyHtoD\n",
        "```\n",
        "\n",
        "**61.98**% of the total time is spent in the **[CUDA memcpy DtoH]** function, and **[CUDA memcpy HtoD]** function. **In total, 98.6% of the total execution time on the GPU is lost in data transfers...** Yes, only 1.36% of time is calculations.\n",
        "\n",
        "But in fact, we don't need to copy all three arrays everytime. We need to copy array a and b **to** the device (c will be set on the device), and we need to copy array c **from** the device to get the results.\n",
        "\n",
        "Below are some examples of how to control data transfers manually:\n",
        "\n",
        "```\n",
        "# Create device array d_a from array a and copy it to the device\n",
        "d_a = cuda.to_device(a)\n",
        "\n",
        "# Alternatively, create device array d_c from array c but DON'T copy it\n",
        "d_c = cuda.to_device(c, copy=False)\n",
        "\n",
        "# Copy the content of device array d_c to host array c\n",
        "d_c.copy_to_host(c)\n",
        "``` \n",
        "\n",
        "Then when calling the kernel function, use the freshly created device arrays rather than the host arrays:\n",
        "```\n",
        "vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
        "```"
      ],
      "metadata": {
        "id": "ibfklGMzlxgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_add_numba_cuda_no_copy(a, b, c):\n",
        "    # your turn\n",
        "        \n",
        "    \n",
        "size = 2**24\n",
        "print(\"Cuda:\")\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)"
      ],
      "metadata": {
        "id": "12kgWZufqxI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you did it right, it should be significantly faster!"
      ],
      "metadata": {
        "id": "sB6P8qdittRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "W6nmFc_iqvno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CGT1rjZA8o62"
      },
      "outputs": [],
      "source": [
        "def vec_add_numba_cuda_no_copy(a, b, c):\n",
        "    d_a = cuda.to_device(a)\n",
        "    d_b = cuda.to_device(b)\n",
        "    d_c = cuda.to_device(c, copy=False)\n",
        "\n",
        "    blocksize = 32\n",
        "    gridsize = int(size / blocksize)\n",
        "\n",
        "    vec_add_numba_cuda[gridsize, blocksize](d_a, d_b, d_c)\n",
        "\n",
        "    d_c.copy_to_host(c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 2**24\n",
        "print(\"Cuda:\")\n",
        "blocksize = 32\n",
        "gridsize = int(size / blocksize)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M482Um5qxzI3",
        "outputId": "81c38af7-b2e9-434f-872f-8e74322d4496"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda:\n",
            "Size:  16777216  elapsed time:  0.06343746185302734  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.06316232681274414  checksum =  43100.967578660464\n",
            "Cuda no copy:\n",
            "Size:  16777216  elapsed time:  0.1107940673828125  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.03498268127441406  checksum =  43100.967578660464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "==9108== Profiling application: python vec_add.py\n",
        "==9108== Profiling result:\n",
        "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
        " GPU activities:   67.61%  49.289ms         2  24.644ms  24.631ms  24.658ms  [CUDA memcpy DtoH]\n",
        "                   29.98%  21.857ms         4  5.4642ms  5.4225ms  5.5229ms  [CUDA memcpy HtoD]\n",
        "                    2.41%  1.7536ms         2  876.79us  876.16us  877.41us  cudapy::__main__::dot_numba_cuda_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
        "      API calls:   50.14%  76.012ms         1  76.012ms  76.012ms  76.012ms  cuDevicePrimaryCtxRetain\n",
        "                   34.52%  52.337ms         2  26.168ms  26.138ms  26.199ms  cuMemcpyDtoH\n",
        "                   14.41%  21.845ms         4  5.4613ms  5.4113ms  5.4877ms  cuMemcpyHtoD\n",
        "```\n",
        "\n",
        "We have doubled the time spent on calculations! The whole execution is a factor 2x faster!"
      ],
      "metadata": {
        "id": "bzEHZwXusmBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Tiling"
      ],
      "metadata": {
        "id": "qRQkWxRqtHND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuda **streams** allow concurrency of execution on a single device within a given context. Queued work items in the same stream execute sequentially, but work items in different streams may execute concurrently. Most operations involving a CUDA device can be performed asynchronously using streams, including data transfers and kernel execution.\n",
        "\n",
        "In this example, computation time is too small to get any benefit, however we can try overlapping data transfers.\n",
        "\n",
        "This is done with a technique called **Tiling**, where we split the work into smaller work items (also called chunks).\n",
        "\n",
        "Here the code is given as an example, and you will not see any benefit in google colab. On a newer GPU, this will be 20 to 40% faster."
      ],
      "metadata": {
        "id": "aGkBa2F2tyFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV_QrPMW8o62",
        "outputId": "34e1a7f2-8b6a-45ed-9719-9ae62cf2cabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda:\n",
            "Size:  16777216  elapsed time:  0.09028387069702148  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.06522536277770996  checksum =  43100.967578660464\n",
            "Cuda no copy:\n",
            "Size:  16777216  elapsed time:  0.037339210510253906  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.03559446334838867  checksum =  43100.967578660464\n",
            "Cuda tiling:\n",
            "Size:  16777216  elapsed time:  0.1687793731689453  checksum =  43100.967578660464\n",
            "Size:  16777216  elapsed time:  0.08616900444030762  checksum =  43100.967578660464\n"
          ]
        }
      ],
      "source": [
        "def vec_add_numba_cuda_tiling(a, b, c):\n",
        "    nChunks = 8\n",
        "    chunkSize = int(size / nChunks)\n",
        "\n",
        "    for i in range(nChunks):\n",
        "        stream = cuda.stream()\n",
        "        \n",
        "        begin = i * chunkSize\n",
        "        end = begin + chunkSize\n",
        "\n",
        "        d_a = cuda.to_device(a[begin:end], stream=stream)\n",
        "        d_b = cuda.to_device(b[begin:end], stream=stream)\n",
        "        d_c = cuda.to_device(c[begin:end], stream=stream, copy=False)\n",
        "\n",
        "        blocksize = 32\n",
        "        gridsize = int(size / blocksize)\n",
        "\n",
        "        #print(\"ChunkSize = \", chunkSize, \" tpb = \", TPB, \" gridsize = \", gridsize)\n",
        "        vec_add_numba_cuda[gridsize, blocksize, stream](d_a, d_b, d_c)\n",
        "\n",
        "        d_c.copy_to_host(c[begin:end], stream=stream)\n",
        "        \n",
        "    \n",
        "size = 2**24\n",
        "print(\"Cuda:\")\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "time_and_check(vec_add_numba_cuda[gridsize, blocksize], size)\n",
        "print(\"Cuda no copy:\")\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "time_and_check(vec_add_numba_cuda_no_copy, size)\n",
        "print(\"Cuda tiling:\")\n",
        "time_and_check(vec_add_numba_cuda_tiling, size)\n",
        "time_and_check(vec_add_numba_cuda_tiling, size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SICxTMkZg4Eh"
      ],
      "name": "vec_add_exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
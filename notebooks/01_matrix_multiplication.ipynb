{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlF5UEfo09s1"
   },
   "source": [
    "## What CPU and GPU am I using?\n",
    "\n",
    "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX13Iezg0boS",
    "outputId": "c0afee97-cb86-42a5-8657-074790be9a40"
   },
   "outputs": [],
   "source": [
    "!echo \"CPU:\"\n",
    "!cat /proc/cpuinfo | grep name\n",
    "!echo \"GPU:\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNwcfvwf0bPy"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub9rl8vskYHm"
   },
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "We start by importing numpy and numba. We also define a checksum function as a fast checking mechanism. **In this case, we use as checksum the sum of all elements divided by N^2, where N is the dimension of the matrix. Other types of checksum could be implemented.**\n",
    "\n",
    "**We will assume that we have a square matrix (N x N).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnpmPSdikYHn"
   },
   "outputs": [],
   "source": [
    "# conda install numba cudatoolkit=10.1\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numba import jit,njit,prange,cuda, types, float32\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Randomize between -10, 10\n",
    "def randomize_array(size):\n",
    "    return 10.0 * 1.0 * (rand(size) - 0.5)\n",
    "\n",
    "# Randomize two matrices a and b and set matrix c to zeros\n",
    "def init(size):\n",
    "    seed(1)\n",
    "    size_sq = size*size\n",
    "    a = np.array(randomize_array(size_sq), dtype=np.float32)\n",
    "    b = np.array(randomize_array(size_sq), dtype=np.float32)\n",
    "    c = np.zeros(size_sq, dtype=np.float32)\n",
    "    return a, b, c\n",
    "\n",
    "# Compile and run the check in parallel with Numba\n",
    "# (we do this to keep this part fast, but it is not the main objective of the execise)\n",
    "@njit(parallel = True)\n",
    "def check(c):\n",
    "    size = len(c)\n",
    "    sum = 0.0\n",
    "    for i in prange(size):\n",
    "        sum += c[i]\n",
    "    return sum\n",
    "\n",
    "# Helper function\n",
    "def time_and_check(matrix_multiply_func, size):\n",
    "    a, b, c = init(size)\n",
    "\n",
    "    start = time.time()\n",
    "    matrix_multiply_func(a, b, c, size)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
    "\n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEriGEEkkYHo"
   },
   "source": [
    "### Interpreted Matrix Multiplication\n",
    "\n",
    "Next, we implement the standard matrix-multiplication algorithm. With the current matrix size N=256*256, it takes about 15s to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kAyrdKLkYHp"
   },
   "outputs": [],
   "source": [
    "def mat_mul_base(a,b,c,size):\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            for k in range(size):\n",
    "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoHyZhoCpCcZ",
    "outputId": "ece18691-a657-4827-c905-3e310ef5fb04"
   },
   "outputs": [],
   "source": [
    "time_and_check(mat_mul_base, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7bY4qLFkYHq"
   },
   "source": [
    "### Compiled Matrix Multiplication\n",
    "\n",
    "Python is an interpreted language, which means standard loops like these are very, very slow...\n",
    "\n",
    "Fortunately, we can use numba to compile this kernel just-in-time. The performance will be comparable to C or FORTRAN code!\n",
    "It should take less than a second with the same matrix size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS5sAoMPkYHq"
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def mat_mul_numba(a,b,c,size):\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            for k in range(size):\n",
    "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cO2ecdcKtACm",
    "outputId": "a7f38521-a4ff-4cd3-f1b0-7efd1044fb42"
   },
   "outputs": [],
   "source": [
    "time_and_check(mat_mul_numba, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6AkAoffkYHq"
   },
   "source": [
    "We are going to optimize this code a lot. So from now on, we will use a much larger matrix size. Let's try again. This should take less than 1 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuA8YGBwkYHq",
    "outputId": "a41d78f2-9ec4-4e1b-8fc9-9d4a62be6ee0"
   },
   "outputs": [],
   "source": [
    "time_and_check(mat_mul_numba, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYAYnby5kYHr"
   },
   "source": [
    "### Cache efficiency\n",
    "\n",
    "The standard implementation that we have above, with loops on i, j, k is actually not very cache-friendly. It accesses memory cells that are always very far apart in memory. A well-known trick is to switch the k and j loop so that most memory accesses become continous in memory for matrix c and b. This **greatly** improves cache-efficiency, and performance. See by yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnWQ-YpWkYHr"
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def mat_mul_numba_opt(a,b,c,size):\n",
    "    for i in range(size):\n",
    "        for k in range(size):  # We just switched j and k\n",
    "            for j in range(size):\n",
    "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK6d-dzIpLv3",
    "outputId": "b7a734b3-0d03-4a95-8906-6b42988e0885"
   },
   "outputs": [],
   "source": [
    "time_and_check(mat_mul_numba_opt, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJe2DVelDxfH",
    "outputId": "661f3fa8-effe-4683-adb3-35e6612812a3"
   },
   "outputs": [],
   "source": [
    "naive = []\n",
    "efficient = []\n",
    "sizes = np.arange(256, 2304, 256)\n",
    "    \n",
    "for size in sizes:\n",
    "    naive.append(time_and_check(mat_mul_numba, size))\n",
    "    efficient.append(time_and_check(mat_mul_numba_opt, size))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(sizes, naive, marker='o', markevery=1, label='Naive')\n",
    "plt.plot(sizes, efficient, marker='x', markevery=1, label='Efficient')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Execution time (s)')\n",
    "plt.draw()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnjHHtaRkYHs"
   },
   "source": [
    "### Parallelization on CPU\n",
    "\n",
    "**Note: Google Collab only let you use one CPU, so you will not benefit from parallelization.**\n",
    "\n",
    "To have a fair comparison with the GPU code that we are going to see in the next section, we can parallelize the CPU code. With 8 CPU cores, the code sould run ~8 times faster.\n",
    "This will be our baseline code to compare against the GPU code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rInwRzMukYHs"
   },
   "outputs": [],
   "source": [
    "@njit(parallel = True)   # we add the option parallel=True to enable threading\n",
    "def mat_mul_numba_opt_parallel(a,b,c,size):\n",
    "    for i in prange(size):  # we use prange instead of range. This is the loop being parallelized.\n",
    "        for k in range(size):\n",
    "            for j in range(size):\n",
    "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejiXURklpOgi",
    "outputId": "72f76679-2a3c-43f9-ce4f-4089295f4725"
   },
   "outputs": [],
   "source": [
    "time_and_check(mat_mul_numba_opt_parallel, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuHvbOVOkYHs"
   },
   "source": [
    "### GPU CUDA implementation: EXERCISE\n",
    "\n",
    "You can try your hands at GPU programming here. Try to answer the following questions and fill the blanks in the code below.\n",
    "\n",
    "**What does a CUDA thread compute?**\n",
    "\n",
    "**Hint: our function has three nested loops. Which loop(s) will be executed in parallel? Which loop(s) can you safely break apart and execute independently in parallel?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer (toggle)</summary>\n",
    "\n",
    "**Each thread will compute the inner k loop of the previous code.**\n",
    "\n",
    "In other word, each thread will compute the dot product between column **i** of matrix **a** and row **j** of matrix **b**.\n",
    "\n",
    "In fact, we don't need the loops on i and j anymore, because **we have one thread per i,j corrdinate**.\n",
    "Note that inside the kernel, a thread knows what cell to compute by fetching its coordinates in the grid. This is the only way to differentiate two threads apart!\n",
    "</details>\n",
    "\n",
    "**What is the block size?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer (toggle)</summary>\n",
    "\n",
    "**Between 4x4 and 32x32.**\n",
    "\n",
    "We must specify the block size, a.k.a **the number of threads per block per dimension**. This must be a **multiple of 32**, and the optimal value will vary by code and by architecture. The upper limit is somwhere around 1024 and 4096, which amounts to 32\\*32 to 64\\*64 for two-dimensional blocks.\n",
    "\n",
    "Remember that we are working with 2D blocks here, because this better suit us. We define a block of threads to be 16\\*16 = 256 threads. Therefore, we will have 256 threads per block.\n",
    "Feel free to try different **threads per block (TPB)** values below! This can greatly affect performance.\n",
    "</details>\n",
    "\n",
    "**What is the grid size?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer (toggle)</summary>\n",
    "\n",
    "**MATRIX_SIZE / BLOCK_SIZE.**\n",
    "\n",
    "Now that we have defined the block size, we can compute the grid size. Remember, we want the grid size to be at least 2048\\*2048 threads. However, the size of the grid must be given in number of blocks. Therefore, we can simply define:\n",
    "- grid_size = mat_size / TPB = 2048 / 16 = 128 blocks in one dimension.\n",
    "\n",
    "And the grid has grid_size\\*grid_size = 128\\*128 = 16384 blocks of threads in total.\n",
    "\n",
    "Note that our grid indeed has exactly 16384 \\* 256  = 2048 \\* 2048 threads.\n",
    "</details>\n",
    "\n",
    "**How to launch the kernel?**\n",
    "\n",
    "We launch the CUDA kernel by passing our number of gridsize and blocksize. The GPU will dispatch the blocks to its streaming multiprocessors where they will be executed in locksteps of 32 threadsalso known as warping.\n",
    "\n",
    "**How fast is it?**\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "o6IMVnH6kYHt",
    "outputId": "198c1599-467b-40bd-dc5a-760aa4d87db3"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def mat_mul_numba_cuda(a,b,c,size):\n",
    "    i=cuda.blockIdx.y*cuda.blockDim.y+cuda.threadIdx.y\n",
    "    j=cuda.blockIdx.x*cuda.blockDim.x+cuda.threadIdx.x\n",
    "    \n",
    "    if i >= size or j >= size:\n",
    "        return\n",
    "    \n",
    "    for k in range(size):\n",
    "        c[i*size+j]+=a[i*size+k]*b[k*size+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "kgA24KCi3U-R",
    "outputId": "e224e863-df86-46cd-e1e6-a07abc4b7f69"
   },
   "outputs": [],
   "source": [
    "# Calling our CUDA kernel is done by passing the number of blocks and number of threads per blocks as follows:\n",
    "#\n",
    "#   mat_mul_numba_cuda[gridsize, blocksize](a, b, c, size)\n",
    "# \n",
    "# We can still use our helper function.\n",
    "\n",
    "for TPB in [4, 8, 16, 32]:\n",
    "    print('TPB = ', TPB)\n",
    "    blocksize = (TPB,TPB)\n",
    "    gridsize = (int(2048/TPB),int(2048/TPB))\n",
    "    time_and_check(mat_mul_numba_cuda[gridsize, blocksize], 2048)\n",
    "    time_and_check(mat_mul_numba_cuda[gridsize, blocksize], 2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVMdwddXkYHt"
   },
   "source": [
    "### Performance\n",
    "\n",
    "This should be pretty fast! Note that the time here includes the time needed to copy matrices a and b to the GPU, and then matrix c back to the cpu. Transferring data is often a major bottlneck on the GPU. The execution of the kernel itself is often much faster.\n",
    "\n",
    "Because the complexity of the matrix-matrix multiplication is O(n^3), the number of computations grow much faster compared to the amount of memory required O(n).\n",
    "\n",
    "**One can expect the GPU to perform better than the CPU with larger matrix sizes.**\n",
    "\n",
    "Let's see if this is true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ihIChclWkYHu",
    "outputId": "cc50b4f3-4382-41ca-be53-453f2c82501f"
   },
   "outputs": [],
   "source": [
    "cpu_times = []\n",
    "gpu_times = []\n",
    "speedups = []\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "sizes = [64, 128, 256, 512, 1024, 2048]#, 4096]\n",
    "for size in sizes:\n",
    "    # CPU\n",
    "    cpu_elapsed_time = time_and_check(mat_mul_numba_opt_parallel, size)\n",
    "    cpu_times.append(cpu_elapsed_time)\n",
    "    \n",
    "    # GPU\n",
    "    blocksize = (TPB,TPB)\n",
    "    gridsize = (int(size/TPB),int(size/TPB))\n",
    "    gpu_elapsed_time = time_and_check(mat_mul_numba_cuda[gridsize, blocksize], size)\n",
    "    gpu_times.append(gpu_elapsed_time)\n",
    "    \n",
    "    speedups.append(cpu_elapsed_time / gpu_elapsed_time)\n",
    "    print('Matrix size:', size, 'GPU speed-up:', cpu_elapsed_time / gpu_elapsed_time)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
    "plt.plot(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Execution Time in seconds (linear scale)')\n",
    "plt.draw()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.semilogy(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
    "plt.semilogy(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
    "plt.legend(loc='upper left')\n",
    "plt.yticks([0.001, 0.01, 0.1, 1.0, 10, 100],[0.001, 0.01, 0.1, 1.0, 10, 100])\n",
    "plt.title('Execution Time in seconds (logarithmic scale)')\n",
    "plt.draw()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(np.arange(0,len(speedups)), speedups, label='speedup')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(np.arange(0,len(speedups)), sizes)\n",
    "plt.title('GPU / CPU speedup')\n",
    "plt.draw()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPoJ3nEH1_tr"
   },
   "source": [
    "### How to interpret the results\n",
    "\n",
    "**First plot:** You should see three plots. The first plot shows the execution time (how long it took to execute the matrix-multiplication in seconds) for different matrix sizes from 64x64 to 2048x2048. As you can see, it took a lot longer for the CPU to compute the largest matrix-sizes. The GPU was always able to finish the computation in under 2 seconds!\n",
    "\n",
    " **Second plot:** This exactly the same thing, but now on a logarithmic scale for the y axis. The logarithmic scale is useful when comparing very different numbers! Indeed, now it is clear that even though the GPU is always fast, it also needs more time to compute larger matrix sizes. In fact, this plot shows that the GPU is about 1 to 2 orders of magnitude (10x-100x) faster than the CPU for this task! \n",
    "\n",
    "**Third plot:** This is the ratio CPU_TIME / GPU_TIME. This is exactly how much faster the GPU is compared to the CPU. It shows that not only the GPU is much faster than the CPU, it is also more efficient with largest matrix sizes! One reason for this is simple: you need a fixed time to send the data to/from the GPU, which scales linearly O(n). However, the amount of computations scales as O(n^3). Simply put: with largest matrix sizes, we spend more time doing computation and less time copying data, which result in a more efficient use of our time, which makes the GPU even better than the CPU for this task!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fLzHmV6Vc9V"
   },
   "source": [
    "## Further optimizations: using shared memory (advanced)\n",
    "\n",
    "Below we define a block matrix-matrix multiplication algorithm to help reduce the number of accesses to the global GPU memory. CUDA provides a fast shared memory for threads in a block to cooperately compute on a task. \n",
    "\n",
    "Matrix a and b are divided into blocks of size `TPB*TPB`, where TPB is also the number of threads per cuda block. Here we define TPB = 16, which means there will be `16*16 = 256` threads per cuda block.\n",
    "\n",
    "In each loop, we do the following:\n",
    "* Because the shared memory is a limited resources, the code preloads a small block at a time from the input arrays. It calls syncthreads() to wait until all threads have finished preloading and before doing the computation on the shared memory.\n",
    "```\n",
    "for i in range(bpg):\n",
    "    # Preload data into shared memory\n",
    "    sA[ty * TPB + tx] = a[(ty + i * TPB) * size + x]\n",
    "    sB[ty * TPB + tx] = b[y * size + tx + i * TPB]\n",
    "```\n",
    "This fills the local shared memory with the corresponding block values from the global GPU memory.\n",
    "\n",
    "* Next, all of our 256 threads will execute the following instructions:\n",
    "```\n",
    "# Computes partial product on the shared memory\n",
    "for j in range(TPB):\n",
    "    tmp += sA[j * TPB + tx] * sB[ty * TPB + j]\n",
    "```\n",
    "which actually compute the block-block multiplication between block sA and sB.  It synchronizes again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration.\n",
    "\n",
    "### Analysis:\n",
    "* Original matrix-matrix algorithm. Each thread compute a `line * column` product, which requires `2 x size` reads and performs `size` operations.\n",
    "* Block matrix algorithm: each thread computes `size / TPB` matrix blocks. For each block, a thread reads `2` (!) values and performs `TPB` operations. In total, a thread will read `(size / TPB) * 2` values and performs `(size / TPB) * TPB = size` operations.\n",
    "\n",
    "| Algorithm           | # Reads from global memory | # multiplications |\n",
    "| ------------------- | -------------------------- |-------------------|\n",
    "| Original algorithm  | `2 * size`                 | `size`            |\n",
    "| Block algorithm     | `2 * (size / TPB)`         | `size`            |\n",
    "\n",
    "**The block algorithm requires less reads from the global memory** because it takes advantage of values already loaded by other threads from the same cuda block. Note that in total, each thread still reads the same number of values , however all the remaining reads are from the shared memory, which is about 100x faster compared to the global memory.\n",
    "\n",
    "In this example, the original cuda matmul algorithm takes 0.5s to execute on while the block algorithm using the shared memory takes 0.4s on average. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfuD-kN0ZCAK"
   },
   "outputs": [],
   "source": [
    "TPB = 8\n",
    "BSIZE = TPB*TPB\n",
    "\n",
    "# this will only work for square matrices that are\n",
    "@cuda.jit\n",
    "def mat_mul_numba_cuda_opt(a, b, c, size):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    ti = cuda.threadIdx.x\n",
    "    tj = cuda.threadIdx.y\n",
    "\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0\n",
    "    for block in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ti * TPB + tj] = a[i * size + tj + block * TPB]\n",
    "        sB[ti * TPB + tj] = b[(ti + block * TPB) * size + j]\n",
    "    \n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for k in range(TPB):\n",
    "            tmp += sA[ti*TPB+k] * sB[k*TPB+tj]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    c[i * size + j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpmqfXo_3hbx",
    "outputId": "03e2a768-8a36-4e4c-94bd-57dceb21331f"
   },
   "outputs": [],
   "source": [
    "size = 4096\n",
    "\n",
    "blocksize = (TPB,TPB)\n",
    "gridsize = (int(size/TPB),int(size/TPB))\n",
    "\n",
    "print(blocksize, gridsize)\n",
    "\n",
    "time_and_check(mat_mul_numba_cuda[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda_opt[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda_opt[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda_opt[gridsize, blocksize],size)\n",
    "time_and_check(mat_mul_numba_cuda_opt[gridsize, blocksize],size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_matrix_multiplication.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gpu-course)",
   "language": "python",
   "name": "gpu-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "01_matrix_multiplication.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlF5UEfo09s1"
      },
      "source": [
        "## What CPU and GPU am I using?\n",
        "\n",
        "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX13Iezg0boS",
        "outputId": "3f29a044-e483-4303-e2f3-0f89e527574c"
      },
      "source": [
        "!echo \"CPU:\"\n",
        "!cat /proc/cpuinfo | grep name\n",
        "!echo \"GPU:\"\n",
        "!nvidia-smi"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU:\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "GPU:\n",
            "Mon Feb  1 22:56:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    30W /  70W |    161MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNwcfvwf0bPy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9rl8vskYHm"
      },
      "source": [
        "## Matrix Multiplication\n",
        "\n",
        "We start by importing numpy and numba. We also define a checksum function as a fast checking mechanism. **In this case, we use as checksum the sum of all elements divided by N^2, where N is the dimension of the matrix. Other types of checksum could be implemented.**\n",
        "\n",
        "**We will assume that we have a square matrix (N x N).** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpmPSdikYHn"
      },
      "source": [
        "# conda install numba cudatoolkit=10.1\n",
        "import time\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "from numba import jit,njit,prange,cuda, types, float32\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "seed(1)\n",
        "def randomize_array(size):\n",
        "    lst = 10.*rand(size)+1.\n",
        "    return lst\n",
        "\n",
        "mat_size = 256\n",
        "mat_size_sq = mat_size*mat_size\n",
        "a = np.array(randomize_array(mat_size_sq), dtype=np.float64)\n",
        "b = np.array(randomize_array(mat_size_sq), dtype=np.float64)\n",
        "\n",
        "# Compile and run the check in parallel with Numba\n",
        "# (we do this to keep this part fast, but it is not the main objective of the execise)\n",
        "@njit(parallel = True)\n",
        "def check(c, N):\n",
        "    NN = N * N\n",
        "    sum = 0.0\n",
        "    for i in prange(NN):\n",
        "        sum += c[i] / NN\n",
        "    return sum"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEriGEEkkYHo"
      },
      "source": [
        "### Interpreted Matrix Multiplication\n",
        "\n",
        "Next, we implement the standard matrix-multiplication algorithm. With the current matrix size N=256*256, it takes about 15s to complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "_kAyrdKLkYHp",
        "outputId": "f3890625-a483-4250-9b91-ddb30136495e"
      },
      "source": [
        "def mat_mul_base(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            for k in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]\n",
        "                \n",
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start = time.time()\n",
        "mat_mul_base(a, b, c, mat_size)\n",
        "end = time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c431be9cda4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_size_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmat_mul_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c431be9cda4f>\u001b[0m in \u001b[0;36mmat_mul_base\u001b[0;34m(a, b, c, size)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_size_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7bY4qLFkYHq"
      },
      "source": [
        "### Compiled Matrix Multiplication\n",
        "\n",
        "Python is an interpreted language, which means standard loops like these are very, very slow...\n",
        "\n",
        "Fortunately, we can use numba to compile this kernel just-in-time. The performance will be comparable to C or FORTRAN code!\n",
        "It should take less than a second with the same matrix size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS5sAoMPkYHq",
        "outputId": "192ba58f-f061-48df-e2c3-d8fc9aab7e1e"
      },
      "source": [
        "@njit\n",
        "def mat_mul_numba(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            for k in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]\n",
        "                \n",
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start = time.time()\n",
        "mat_mul_numba(a, b, c, mat_size)\n",
        "end = time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  0.22360992431640625\n",
            "9217.815409995615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6AkAoffkYHq"
      },
      "source": [
        "We are going to optimize this code a lot. So from now on, we will use a much larger matrix size. Let's try again. This should take less than 1 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuA8YGBwkYHq"
      },
      "source": [
        "mat_size = 2048\n",
        "mat_size_sq = mat_size*mat_size\n",
        "a = np.array(randomize_array(mat_size_sq), dtype=np.float32)\n",
        "b = np.array(randomize_array(mat_size_sq), dtype=np.float32)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKBgKGtAkYHr",
        "outputId": "008c491c-3b6c-4f00-8dd1-99abc704ff2c"
      },
      "source": [
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start = time.time()\n",
        "mat_mul_numba(a, b, c, mat_size)\n",
        "end = time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  28.74476909637451\n",
            "73721.29736233875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYAYnby5kYHr"
      },
      "source": [
        "### Cache efficiency\n",
        "\n",
        "The standard implementation that we have above, with loops on i, j, k is actually not very cache-friendly. It accesses memory cells that are always very far apart in memory. A well-known trick is to switch the k and j loop so that most memory accesses become continous in memory for matrix c and b. This **greatly** improves cache-efficiency, and performance. See by yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnWQ-YpWkYHr",
        "outputId": "d047472b-f457-423f-8fe8-7d615a4c12db"
      },
      "source": [
        "@njit\n",
        "def mat_mul_numba_opt(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for k in range(size):  # We just switched j and k\n",
        "            for j in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]\n",
        "                \n",
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start = time.time()\n",
        "mat_mul_numba_opt(a, b, c, mat_size)\n",
        "end = time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  14.062775373458862\n",
            "73721.29736233875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnjHHtaRkYHs"
      },
      "source": [
        "### Parallelization on CPU\n",
        "\n",
        "**Note: Google Collab only let you use one CPU, so you will not benefit from parallelization.**\n",
        "\n",
        "To have a fair comparison with the GPU code that we are going to see in the next section, we can parallelize the CPU code. With 8 CPU cores, the code sould run ~8 times faster.\n",
        "This will be our baseline code to compare against the GPU code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rInwRzMukYHs",
        "outputId": "71ab840c-fc82-4326-fbd8-2528faa78bd8"
      },
      "source": [
        "@njit(parallel = True)   # we add the option parallel=True to enable threading\n",
        "def mat_mul_numba_parallel(a,b,c,size):\n",
        "    for i in prange(size):  # we use prange instead of range. This is the loop being parallelized.\n",
        "        for k in range(size):\n",
        "            for j in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]\n",
        "                \n",
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start = time.time()\n",
        "mat_mul_numba_parallel(a, b, c, mat_size)\n",
        "end = time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  12.744398832321167\n",
            "73721.29736233875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuHvbOVOkYHs"
      },
      "source": [
        "### GPU CUDA implementation\n",
        "\n",
        "Below is the CUDA equivalent of the previous code.\n",
        "\n",
        "**How many threads?**\n",
        "\n",
        "We create a grid of 2048*2048 threads, in other words, we have one thread per matrix cell.\n",
        "\n",
        "The GPU does not have enough CUDA cores to execute all of our 2048*2048 threads in parallel. But we don't care. The GPU will take care of this and schedule the work accordingly.\n",
        "\n",
        "**What does a thread compute?**\n",
        "\n",
        "Each thread will compute the inner k loop of the previous code. In other word, each thread will compute the dot product between column **i** of matrix **a** and row **j** of matrix **b**.\n",
        "\n",
        "In fact, we don't need the loops on i and j anymore, because we have one thread per i,j corrdinate.\n",
        "Note that inside the kernel, a thread knows what cell to compute by fetching its coordinates in the grid. This is the only way to differentiate two threads apart!\n",
        "\n",
        "**What is the block size?**\n",
        "\n",
        "We must specify the block size, a.k.a **the number of threads per block**. This must be a **multiple of 32**, and the optimal value will vary by code and by architecture. The upper limit is somwhere around 1024 and 4096, which amounts to 32\\*32 to 64\\*64 for two-dimensional blocks.\n",
        "\n",
        "Here, we define a block of threads to be 16\\*16 = 256 threads. Therefore, we will have 256 threads per block.\n",
        "Feel free to try different threads per block (TPB) values below! This can greatly affect performance.\n",
        "\n",
        "**What is the grid size?**\n",
        "\n",
        "Now that we have defined the block size, we can compute the grid size. Remember, we want the grid size to be at least 2048\\*2048 threads. However, the size of the grid must be given in number of blocks. Therefore, we can simply define:\n",
        "- grid_size = mat_size / TPB = 2048 / 16 = 128 blocks in one dimension.\n",
        "\n",
        "And the grid has grid_size\\*grid_size = 128\\*128 = 16384 blocks of threads in total.\n",
        "\n",
        "Note that our grid indeed has exactly 16384 \\* 256  = 2048 \\* 2048 threads.\n",
        "\n",
        "**How to launch the kernel?**\n",
        "\n",
        "We launch the CUDA kernel by passing our number of blockspergrid and threadsperblock. The GPU will dispatch the blocks to its streaming multiprocessors where they will be executed in locksteps of 32 threads (warping).\n",
        "\n",
        "**How fast is it?**\n",
        "\n",
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6IMVnH6kYHt"
      },
      "source": [
        "@cuda.jit\n",
        "def mat_mul_numba_cuda(a,b,c,size):\n",
        "    i=cuda.blockIdx.y*cuda.blockDim.y+cuda.threadIdx.y\n",
        "    j=cuda.blockIdx.x*cuda.blockDim.x+cuda.threadIdx.x\n",
        "    \n",
        "    if i >= size or j >= size:\n",
        "        return\n",
        "    \n",
        "    for k in range(size):\n",
        "        c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgA24KCi3U-R",
        "outputId": "73a228e0-cadb-4976-bc0f-ba91c1e02697"
      },
      "source": [
        "TPB = 16\n",
        "threadsperblock = (TPB,TPB)\n",
        "grid_size = int(np.ceil(mat_size / TPB))\n",
        "blockspergrid = (grid_size,grid_size)\n",
        "\n",
        "c = np.zeros(mat_size_sq, dtype=np.float64)\n",
        "start=time.time()\n",
        "mat_mul_numba_cuda[blockspergrid, threadsperblock](a,b,c,mat_size)\n",
        "end=time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  0.22638988494873047\n",
            "73721.29736043184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVMdwddXkYHt"
      },
      "source": [
        "### Performance\n",
        "\n",
        "This should be pretty fast! Note that the time here includes the time needed to copy matrices a and b to the GPU, and then matrix c back to the cpu. Transferring data is often a major bottlneck on the GPU. The execution of the kernel itself is often much faster.\n",
        "\n",
        "Because the complexity of the matrix-matrix multiplication is O(n^3), the number of computations grow much faster compared to the amount of memory required O(n).\n",
        "\n",
        "**One can expect the GPU to perform better than the CPU with larger matrix sizes.**\n",
        "\n",
        "Let's see if this is true!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "ihIChclWkYHu",
        "outputId": "26809f5a-76f0-4941-f551-5db5d035034a"
      },
      "source": [
        "cpu_times = []\n",
        "gpu_times = []\n",
        "speedups = []\n",
        "\n",
        "TPB = 16\n",
        "threadsperblock = (TPB,TPB)\n",
        "grid_size = int(np.ceil(mat_size / TPB))\n",
        "blockspergrid = (grid_size,grid_size)\n",
        "\n",
        "sizes = [64, 128, 256, 512, 1024, 2048]#, 4096]\n",
        "for mat_size in sizes:\n",
        "    mat_size_sq = mat_size*mat_size\n",
        "    a = np.array(randomize_array(mat_size_sq), dtype=np.float32)\n",
        "    b = np.array(randomize_array(mat_size_sq), dtype=np.float32)\n",
        "    \n",
        "    # CPU\n",
        "    c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "    start = time.time()\n",
        "    mat_mul_numba_parallel(a, b, c, mat_size)\n",
        "    end = time.time()\n",
        "    ctime = end-start\n",
        "    cpu_times.append(ctime)\n",
        "    \n",
        "    # GPU\n",
        "    c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "    start=time.time()\n",
        "    mat_mul_numba_cuda[blockspergrid, threadsperblock](a,b,c,mat_size)\n",
        "    end=time.time()\n",
        "    gtime = end-start\n",
        "    gpu_times.append(gtime)\n",
        "    \n",
        "    speedups.append(ctime/gtime)\n",
        "    print('Matrix size:',mat_size, 'GPU speed-up:',ctime/gtime)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
        "plt.plot(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Execution Time in seconds (linear scale)')\n",
        "plt.draw()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.semilogy(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
        "plt.semilogy(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
        "plt.legend(loc='upper left')\n",
        "plt.yticks([0.001, 0.01, 0.1, 1.0, 10, 100],[0.001, 0.01, 0.1, 1.0, 10, 100])\n",
        "plt.title('Execution Time in seconds (logarithmic scale)')\n",
        "plt.draw()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(np.arange(0,len(speedups)), speedups, label='speedup')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xticks(np.arange(0,len(speedups)), sizes)\n",
        "plt.title('GPU / CPU speedup')\n",
        "plt.draw()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-68d95f0f6c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_size_sq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmat_mul_numba_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mctime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mat_mul_numba_parallel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPoJ3nEH1_tr"
      },
      "source": [
        "### How to interpret the results\n",
        "\n",
        "**First plot:** You should see three plots. The first plot shows the execution time (how long it took to execute the matrix-multiplication in seconds) for different matrix sizes from 64x64 to 2048x2048. As you can see, it took a lot longer for the CPU to compute the largest matrix-sizes. The GPU was always able to finish the computation in under 2 seconds!\n",
        "\n",
        " **Second plot:** This exactly the same thing, but now on a logarithmic scale for the y axis. The logarithmic scale is useful when comparing very different numbers! Indeed, now it is clear that even though the GPU is always fast, it also needs more time to compute larger matrix sizes. In fact, this plot shows that the GPU is about 1 to 2 orders of magnitude (10x-100x) faster than the CPU for this task! \n",
        "\n",
        "**Third plot:** This is the ratio CPU_TIME / GPU_TIME. This is exactly how much faster the GPU is compared to the CPU. It shows that not only the GPU is much faster than the CPU, it is also more efficient with largest matrix sizes! One reason for this is simple: you need a fixed time to send the data to/from the GPU, which scales linearly O(n). However, the amount of computations scales as O(n^3). Simply put: with largest matrix sizes, we spend more time doing computation and less time copying data, which result in a more efficient use of our time, which makes the GPU even better than the CPU for this task!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fLzHmV6Vc9V"
      },
      "source": [
        "## Further optimizations: using shared memory\n",
        "\n",
        "Below we define a block matrix-matrix multiplication algorithm to help reduce the number of accesses to the global GPU memory. CUDA provides a fast shared memory for threads in a block to cooperately compute on a task. \n",
        "\n",
        "Matrix a and b are divided into blocks of size `TPB*TPB`, where TPB is also the number of threads per cuda block. Here we define TPB = 16, which means there will be `16*16 = 256` threads per cuda block.\n",
        "\n",
        "In each loop, we do the following:\n",
        "* Because the shared memory is a limited resources, the code preloads a small block at a time from the input arrays. It calls syncthreads() to wait until all threads have finished preloading and before doing the computation on the shared memory.\n",
        "```\n",
        "for i in range(bpg):\n",
        "    # Preload data into shared memory\n",
        "    sA[ty * TPB + tx] = a[(ty + i * TPB) * size + x]\n",
        "    sB[ty * TPB + tx] = b[y * size + tx + i * TPB]\n",
        "```\n",
        "This fills the local shared memory with the corresponding block values from the global GPU memory.\n",
        "\n",
        "* Next, all of our 256 threads will execute the following instructions:\n",
        "```\n",
        "# Computes partial product on the shared memory\n",
        "for j in range(TPB):\n",
        "    tmp += sA[j * TPB + tx] * sB[ty * TPB + j]\n",
        "```\n",
        "which actually compute the block-block multiplication between block sA and sB.  It synchronizes again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration.\n",
        "\n",
        "### Analysis:\n",
        "* Original matrix-matrix algorithm. Each thread compute a `line * column` product, which requires `2 x size` reads and performs `size` operations.\n",
        "* Block matrix algorithm: each thread computes `size / TPB` matrix blocks. For each block, a thread reads `2` (!) values and performs `TPB` operations. In total, a thread will read `(size / TPB) * 2` values and performs `(size / TPB) * TPB = size` operations.\n",
        "\n",
        "| Algorithm           | # Reads from global memory | # multiplications |\n",
        "| ------------------- | -------------------------- |-------------------|\n",
        "| Original algorithm  | `2 * size`                 | `size`            |\n",
        "| Block algorithm     | `2 * (size / TPB)`         | `size`            |\n",
        "\n",
        "The block algorithm requires less reads from the global memory because it takes advantage of values already loaded by other threads from the same cuda block. Note that in total, each thread still reads the same number of values , however all the remaining reads are from the shared memory, which is about 100x faster compared to the global memory.\n",
        "\n",
        "In this example, the original cuda matmul algorithm takes 0.5s to execute on while the block algorithm using the shared memory takes 0.4s on average. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-1ef3uqWMlA"
      },
      "source": [
        "TPB = 16\n",
        "BSIZE = TPB*TPB\n",
        "\n",
        "# this will only work for square matrices that are\n",
        "@cuda.jit\n",
        "def fast_matmul(a, b, c, size):\n",
        "    # Define an array in the shared memory\n",
        "    # The size and type of the arrays must be known at compile time\n",
        "    sA = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
        "    sB = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
        "\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    bpg = cuda.gridDim.x    # blocks per grid\n",
        "\n",
        "    # Each thread computes one element in the result matrix.\n",
        "    # The dot product is chunked into dot products of TPB-long vectors.\n",
        "    tmp = 0.\n",
        "    for i in range(bpg):\n",
        "        # Preload data into shared memory\n",
        "        sA[ty * TPB + tx] = a[(ty + i * TPB) * size + x]\n",
        "        sB[ty * TPB + tx] = b[y * size + tx + i * TPB]\n",
        "\n",
        "        # Wait until all threads finish preloading\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        # Computes partial product on the shared memory\n",
        "        for j in range(TPB):\n",
        "            tmp += sA[j * TPB + tx] * sB[ty * TPB + j]\n",
        "\n",
        "        # Wait until all threads finish computing\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    c[y * size + x] = tmp"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpmqfXo_3hbx",
        "outputId": "2e194446-5e20-41e7-fcb1-89dde6523228"
      },
      "source": [
        "threadsperblock = (TPB,TPB)\n",
        "grid_size = int(np.ceil(mat_size / TPB))\n",
        "blockspergrid = (grid_size,grid_size)\n",
        "\n",
        "c = np.zeros(mat_size_sq, dtype=np.float32)\n",
        "start=time.time()\n",
        "fast_matmul[blockspergrid, threadsperblock](a,b,c,mat_size)\n",
        "end=time.time()\n",
        "\n",
        "print('Elapsed time: ',end-start)\n",
        "print(check(c, mat_size))"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed time:  0.18884849548339844\n",
            "73721.27649824694\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
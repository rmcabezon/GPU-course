{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "01_matrix_multiplication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlF5UEfo09s1"
      },
      "source": [
        "## What CPU and GPU am I using?\n",
        "\n",
        "Before we start, lets check what processor and GPU we will be using. Performance can vary a lot depending on which model we are using. Google Collab does not allow us to choose the model, but it is free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX13Iezg0boS",
        "outputId": "c0afee97-cb86-42a5-8657-074790be9a40"
      },
      "source": [
        "!echo \"CPU:\"\n",
        "!cat /proc/cpuinfo | grep name\n",
        "!echo \"GPU:\"\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU:\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "GPU:\n",
            "Wed Jan 26 08:47:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNwcfvwf0bPy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9rl8vskYHm"
      },
      "source": [
        "## Matrix Multiplication\n",
        "\n",
        "We start by importing numpy and numba. We also define a checksum function as a fast checking mechanism. **In this case, we use as checksum the sum of all elements divided by N^2, where N is the dimension of the matrix. Other types of checksum could be implemented.**\n",
        "\n",
        "**We will assume that we have a square matrix (N x N).** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpmPSdikYHn"
      },
      "source": [
        "# conda install numba cudatoolkit=10.1\n",
        "import time\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from numpy.random import rand\n",
        "from numba import jit,njit,prange,cuda, types, float32\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Randomize between -10, 10\n",
        "def randomize_array(size):\n",
        "    return 1.0 * (rand(size) - 0.5)\n",
        "\n",
        "# Randomize two matrices a and b and set matrix c to zeros\n",
        "def init(size):\n",
        "    seed(1)\n",
        "    size_sq = size*size\n",
        "    a = np.array(randomize_array(size_sq), dtype=np.float32)\n",
        "    b = np.array(randomize_array(size_sq), dtype=np.float32)\n",
        "    c = np.zeros(size_sq, dtype=np.float32)\n",
        "    return a, b, c\n",
        "\n",
        "# Compile and run the check in parallel with Numba\n",
        "# (we do this to keep this part fast, but it is not the main objective of the execise)\n",
        "@njit(parallel = True)\n",
        "def check(c):\n",
        "    size = len(c)\n",
        "    sum = 0.0\n",
        "    for i in prange(size):\n",
        "        sum += c[i]\n",
        "    return sum\n",
        "\n",
        "# Helper function\n",
        "def time_and_check(matrix_multiply_func, size):\n",
        "    a, b, c = init(size)\n",
        "\n",
        "    start = time.time()\n",
        "    matrix_multiply_func(a, b, c, size)\n",
        "    end = time.time()\n",
        "\n",
        "    print('Size: ', size, ' elapsed time: ',end-start, ' checksum = ', check(c))\n",
        "\n",
        "    return end-start"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEriGEEkkYHo"
      },
      "source": [
        "### Interpreted Matrix Multiplication\n",
        "\n",
        "Next, we implement the standard matrix-multiplication algorithm. With the current matrix size N=256*256, it takes about 15s to complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kAyrdKLkYHp"
      },
      "source": [
        "def mat_mul_base(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            for k in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoHyZhoCpCcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece18691-a657-4827-c905-3e310ef5fb04"
      },
      "source": [
        "time_and_check(mat_mul_base, 256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  256  elapsed time:  58.489418745040894  checksum =  0.05421780673996324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58.489418745040894"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7bY4qLFkYHq"
      },
      "source": [
        "### Compiled Matrix Multiplication\n",
        "\n",
        "Python is an interpreted language, which means standard loops like these are very, very slow...\n",
        "\n",
        "Fortunately, we can use numba to compile this kernel just-in-time. The performance will be comparable to C or FORTRAN code!\n",
        "It should take less than a second with the same matrix size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS5sAoMPkYHq"
      },
      "source": [
        "@njit\n",
        "def mat_mul_numba(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            for k in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO2ecdcKtACm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f38521-a4ff-4cd3-f1b0-7efd1044fb42"
      },
      "source": [
        "time_and_check(mat_mul_numba, 256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  256  elapsed time:  0.3412501811981201  checksum =  0.05421780673996324\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3412501811981201"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6AkAoffkYHq"
      },
      "source": [
        "We are going to optimize this code a lot. So from now on, we will use a much larger matrix size. Let's try again. This should take less than 1 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuA8YGBwkYHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41d78f2-9ec4-4e1b-8fc9-9d4a62be6ee0"
      },
      "source": [
        "time_and_check(mat_mul_numba, 2048)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  2048  elapsed time:  56.37022852897644  checksum =  -0.3384584113167396\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56.37022852897644"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYAYnby5kYHr"
      },
      "source": [
        "### Cache efficiency\n",
        "\n",
        "The standard implementation that we have above, with loops on i, j, k is actually not very cache-friendly. It accesses memory cells that are always very far apart in memory. A well-known trick is to switch the k and j loop so that most memory accesses become continous in memory for matrix c and b. This **greatly** improves cache-efficiency, and performance. See by yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnWQ-YpWkYHr"
      },
      "source": [
        "@njit\n",
        "def mat_mul_numba_opt(a,b,c,size):\n",
        "    for i in range(size):\n",
        "        for k in range(size):  # We just switched j and k\n",
        "            for j in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK6d-dzIpLv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a734b3-0d03-4a95-8906-6b42988e0885"
      },
      "source": [
        "time_and_check(mat_mul_numba_opt, 2048)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  2048  elapsed time:  21.99462866783142  checksum =  -0.3384584113167396\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.99462866783142"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnjHHtaRkYHs"
      },
      "source": [
        "### Parallelization on CPU\n",
        "\n",
        "**Note: Google Collab only let you use one CPU, so you will not benefit from parallelization.**\n",
        "\n",
        "To have a fair comparison with the GPU code that we are going to see in the next section, we can parallelize the CPU code. With 8 CPU cores, the code sould run ~8 times faster.\n",
        "This will be our baseline code to compare against the GPU code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rInwRzMukYHs"
      },
      "source": [
        "@njit(parallel = True)   # we add the option parallel=True to enable threading\n",
        "def mat_mul_numba_opt_parallel(a,b,c,size):\n",
        "    for i in prange(size):  # we use prange instead of range. This is the loop being parallelized.\n",
        "        for k in range(size):\n",
        "            for j in range(size):\n",
        "                c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejiXURklpOgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f76679-2a3c-43f9-ce4f-4089295f4725"
      },
      "source": [
        "time_and_check(mat_mul_numba_opt_parallel, 2048)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  2048  elapsed time:  19.533366918563843  checksum =  -0.3384584113167396\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19.533366918563843"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuHvbOVOkYHs"
      },
      "source": [
        "### GPU CUDA implementation: EXERCISE\n",
        "\n",
        "You can try your hands at GPU programming here. Try to answer the following questions and fill the blanks in the code below.\n",
        "\n",
        "**What does a CUDA thread compute?**\n",
        "\n",
        "**Hint: our function has three nested loops. Which loop(s) will be executed in parallel? Which loop(s) can you safely break apart and execute independently in parallel?**\n",
        "\n",
        "Each thread will compute the inner k loop of the previous code. In other word, each thread will compute the dot product between column **i** of matrix **a** and row **j** of matrix **b**.\n",
        "\n",
        "In fact, we don't need the loops on i and j anymore, because **we have one thread per i,j corrdinate**.\n",
        "Note that inside the kernel, a thread knows what cell to compute by fetching its coordinates in the grid. This is the only way to differentiate two threads apart!\n",
        "\n",
        "**What is the block size?**\n",
        "\n",
        "We must specify the block size, a.k.a **the number of threads per block**. This must be a **multiple of 32**, and the optimal value will vary by code and by architecture. The upper limit is somwhere around 1024 and 4096, which amounts to 32\\*32 to 64\\*64 for two-dimensional blocks.\n",
        "\n",
        "Remember that we are working with 2D blocks here, because this better suit us. We define a block of threads to be 16\\*16 = 256 threads. Therefore, we will have 256 threads per block.\n",
        "Feel free to try different threads per block (TPB) values below! This can greatly affect performance.\n",
        "\n",
        "**What is the grid size?**\n",
        "\n",
        "Now that we have defined the block size, we can compute the grid size. Remember, we want the grid size to be at least 2048\\*2048 threads. However, the size of the grid must be given in number of blocks. Therefore, we can simply define:\n",
        "- grid_size = mat_size / TPB = 2048 / 16 = 128 blocks in one dimension.\n",
        "\n",
        "And the grid has grid_size\\*grid_size = 128\\*128 = 16384 blocks of threads in total.\n",
        "\n",
        "Note that our grid indeed has exactly 16384 \\* 256  = 2048 \\* 2048 threads.\n",
        "\n",
        "**How to launch the kernel?**\n",
        "\n",
        "We launch the CUDA kernel by passing our number of blockspergrid and threadsperblock. The GPU will dispatch the blocks to its streaming multiprocessors where they will be executed in locksteps of 32 threadsalso known as warping.\n",
        "\n",
        "**How fast is it?**\n",
        "\n",
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6IMVnH6kYHt"
      },
      "source": [
        "@cuda.jit\n",
        "def mat_mul_numba_cuda(a,b,c,size):\n",
        "    i=cuda.blockIdx.y*cuda.blockDim.y+cuda.threadIdx.y\n",
        "    j=cuda.blockIdx.x*cuda.blockDim.x+cuda.threadIdx.x\n",
        "    \n",
        "    if i >= size or j >= size:\n",
        "        return\n",
        "    \n",
        "    for k in range(size):\n",
        "        c[i*size+j]+=a[i*size+k]*b[k*size+j]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "kgA24KCi3U-R",
        "outputId": "e224e863-df86-46cd-e1e6-a07abc4b7f69"
      },
      "source": [
        "# Calling our CUDA kernel is done by passing the number of blocks and number of threads per blocks as follows:\n",
        "#\n",
        "#   mat_mul_numba_cuda[blockspergrid, threadsperblock](a, b, c, size)\n",
        "# \n",
        "# We can still use our helper function.\n",
        "\n",
        "for TPB in [4, 8, 16, 32]:\n",
        "  print('TPB = ', TPB)\n",
        "  threadsperblock = (TPB,TPB)\n",
        "  blockspergrid = (int(2048/TPB),int(2048/TPB))\n",
        "  time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock], 2048)\n",
        "  time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock], 2048)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPB =  4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CudaAPIError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-719f69782219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mthreadsperblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTPB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTPB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mblockspergrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTPB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTPB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtime_and_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mul_numba_cuda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblockspergrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadsperblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mtime_and_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mul_numba_cuda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblockspergrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadsperblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-ff4ac4300a3b>\u001b[0m in \u001b[0;36mtime_and_check\u001b[0;34m(matrix_multiply_func, size)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmatrix_multiply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0;32m--> 770\u001b[0;31m                                     self.stream, self.sharedmem)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    859\u001b[0m         argtypes = tuple(\n\u001b[1;32m    860\u001b[0m             [self.typingctx.resolve_argument_type(a) for a in args])\n\u001b[0;32m--> 861\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                 \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mForce\u001b[0m \u001b[0mbinding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \"\"\"\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;31m# Link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mlinker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_registers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_registers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_ptx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_registers)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcu_link_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m         driver.cuLinkCreate(len(raw_keys), option_keys, option_vals,\n\u001b[0;32m-> 2086\u001b[0;31m                             byref(self.handle))\n\u001b[0m\u001b[1;32m   2087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m         \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuLinkDestroy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call driver api: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaAPIError\u001b[0m: [700] Call to cuLinkCreate results in UNKNOWN_CUDA_ERROR"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVMdwddXkYHt"
      },
      "source": [
        "### Performance\n",
        "\n",
        "This should be pretty fast! Note that the time here includes the time needed to copy matrices a and b to the GPU, and then matrix c back to the cpu. Transferring data is often a major bottlneck on the GPU. The execution of the kernel itself is often much faster.\n",
        "\n",
        "Because the complexity of the matrix-matrix multiplication is O(n^3), the number of computations grow much faster compared to the amount of memory required O(n).\n",
        "\n",
        "**One can expect the GPU to perform better than the CPU with larger matrix sizes.**\n",
        "\n",
        "Let's see if this is true!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ihIChclWkYHu",
        "outputId": "cc50b4f3-4382-41ca-be53-453f2c82501f"
      },
      "source": [
        "cpu_times = []\n",
        "gpu_times = []\n",
        "speedups = []\n",
        "\n",
        "TPB = 16\n",
        "\n",
        "sizes = [64, 128, 256, 512, 1024, 2048]#, 4096]\n",
        "for size in sizes:\n",
        "    # CPU\n",
        "    cpu_elapsed_time = time_and_check(mat_mul_numba_opt_parallel, size)\n",
        "    cpu_times.append(cpu_elapsed_time)\n",
        "    \n",
        "    # GPU\n",
        "    threadsperblock = (TPB,TPB)\n",
        "    blockspergrid = (int(size/TPB),int(size/TPB))\n",
        "    gpu_elapsed_time = time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock], size)\n",
        "    gpu_times.append(gpu_elapsed_time)\n",
        "    \n",
        "    speedups.append(cpu_elapsed_time / gpu_elapsed_time)\n",
        "    print('Matrix size:', size, 'GPU speed-up:', cpu_elapsed_time / gpu_elapsed_time)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
        "plt.plot(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Execution Time in seconds (linear scale)')\n",
        "plt.draw()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.semilogy(sizes, cpu_times, marker='o', markevery=1, label='CPU')\n",
        "plt.semilogy(sizes, gpu_times, marker='x', markevery=1, label='GPU')\n",
        "plt.legend(loc='upper left')\n",
        "plt.yticks([0.001, 0.01, 0.1, 1.0, 10, 100],[0.001, 0.01, 0.1, 1.0, 10, 100])\n",
        "plt.title('Execution Time in seconds (logarithmic scale)')\n",
        "plt.draw()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(np.arange(0,len(speedups)), speedups, label='speedup')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xticks(np.arange(0,len(speedups)), sizes)\n",
        "plt.title('GPU / CPU speedup')\n",
        "plt.draw()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  64  elapsed time:  0.006211042404174805  checksum =  1.080207280245304\n",
            "Size:  64  elapsed time:  0.00532078742980957  checksum =  1.080207280245304\n",
            "Matrix size: 64 GPU speed-up: 1.1673163955728816\n",
            "Size:  128  elapsed time:  0.004765033721923828  checksum =  -1.8073311286873377\n",
            "Size:  128  elapsed time:  0.004845619201660156  checksum =  -1.8073311286873377\n",
            "Matrix size: 128 GPU speed-up: 0.9833694154693958\n",
            "Size:  256  elapsed time:  0.03920149803161621  checksum =  0.05421780673996324\n",
            "Size:  256  elapsed time:  0.010982990264892578  checksum =  0.05421780673996324\n",
            "Matrix size: 256 GPU speed-up: 3.5692918855555074\n",
            "Size:  512  elapsed time:  0.32788515090942383  checksum =  1.6381389885572863\n",
            "Size:  512  elapsed time:  0.012423515319824219  checksum =  1.6381389885572863\n",
            "Matrix size: 512 GPU speed-up: 26.392300606432794\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b920c29cb36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcpu_elapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_and_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_mul_numba_opt_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcpu_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_elapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a29052065707>\u001b[0m in \u001b[0;36mtime_and_check\u001b[0;34m(matrix_multiply_func, size)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmatrix_multiply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPoJ3nEH1_tr"
      },
      "source": [
        "### How to interpret the results\n",
        "\n",
        "**First plot:** You should see three plots. The first plot shows the execution time (how long it took to execute the matrix-multiplication in seconds) for different matrix sizes from 64x64 to 2048x2048. As you can see, it took a lot longer for the CPU to compute the largest matrix-sizes. The GPU was always able to finish the computation in under 2 seconds!\n",
        "\n",
        " **Second plot:** This exactly the same thing, but now on a logarithmic scale for the y axis. The logarithmic scale is useful when comparing very different numbers! Indeed, now it is clear that even though the GPU is always fast, it also needs more time to compute larger matrix sizes. In fact, this plot shows that the GPU is about 1 to 2 orders of magnitude (10x-100x) faster than the CPU for this task! \n",
        "\n",
        "**Third plot:** This is the ratio CPU_TIME / GPU_TIME. This is exactly how much faster the GPU is compared to the CPU. It shows that not only the GPU is much faster than the CPU, it is also more efficient with largest matrix sizes! One reason for this is simple: you need a fixed time to send the data to/from the GPU, which scales linearly O(n). However, the amount of computations scales as O(n^3). Simply put: with largest matrix sizes, we spend more time doing computation and less time copying data, which result in a more efficient use of our time, which makes the GPU even better than the CPU for this task!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fLzHmV6Vc9V"
      },
      "source": [
        "## Further optimizations: using shared memory (advanced)\n",
        "\n",
        "Below we define a block matrix-matrix multiplication algorithm to help reduce the number of accesses to the global GPU memory. CUDA provides a fast shared memory for threads in a block to cooperately compute on a task. \n",
        "\n",
        "Matrix a and b are divided into blocks of size `TPB*TPB`, where TPB is also the number of threads per cuda block. Here we define TPB = 16, which means there will be `16*16 = 256` threads per cuda block.\n",
        "\n",
        "In each loop, we do the following:\n",
        "* Because the shared memory is a limited resources, the code preloads a small block at a time from the input arrays. It calls syncthreads() to wait until all threads have finished preloading and before doing the computation on the shared memory.\n",
        "```\n",
        "for i in range(bpg):\n",
        "    # Preload data into shared memory\n",
        "    sA[ty * TPB + tx] = a[(ty + i * TPB) * size + x]\n",
        "    sB[ty * TPB + tx] = b[y * size + tx + i * TPB]\n",
        "```\n",
        "This fills the local shared memory with the corresponding block values from the global GPU memory.\n",
        "\n",
        "* Next, all of our 256 threads will execute the following instructions:\n",
        "```\n",
        "# Computes partial product on the shared memory\n",
        "for j in range(TPB):\n",
        "    tmp += sA[j * TPB + tx] * sB[ty * TPB + j]\n",
        "```\n",
        "which actually compute the block-block multiplication between block sA and sB.  It synchronizes again after the computation to ensure all threads have finished with the data in shared memory before overwriting it in the next loop iteration.\n",
        "\n",
        "### Analysis:\n",
        "* Original matrix-matrix algorithm. Each thread compute a `line * column` product, which requires `2 x size` reads and performs `size` operations.\n",
        "* Block matrix algorithm: each thread computes `size / TPB` matrix blocks. For each block, a thread reads `2` (!) values and performs `TPB` operations. In total, a thread will read `(size / TPB) * 2` values and performs `(size / TPB) * TPB = size` operations.\n",
        "\n",
        "| Algorithm           | # Reads from global memory | # multiplications |\n",
        "| ------------------- | -------------------------- |-------------------|\n",
        "| Original algorithm  | `2 * size`                 | `size`            |\n",
        "| Block algorithm     | `2 * (size / TPB)`         | `size`            |\n",
        "\n",
        "**The block algorithm requires less reads from the global memory** because it takes advantage of values already loaded by other threads from the same cuda block. Note that in total, each thread still reads the same number of values , however all the remaining reads are from the shared memory, which is about 100x faster compared to the global memory.\n",
        "\n",
        "In this example, the original cuda matmul algorithm takes 0.5s to execute on while the block algorithm using the shared memory takes 0.4s on average. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TPB = 8\n",
        "BSIZE = TPB*TPB\n",
        "\n",
        "# this will only work for square matrices that are\n",
        "@cuda.jit\n",
        "def mat_mul_numba_cuda_opt(a, b, c, size):\n",
        "    # Define an array in the shared memory\n",
        "    # The size and type of the arrays must be known at compile time\n",
        "    sA = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
        "    sB = cuda.shared.array(shape=(BSIZE), dtype=float32)\n",
        "\n",
        "    i, j = cuda.grid(2)\n",
        "\n",
        "    ti = cuda.threadIdx.x\n",
        "    tj = cuda.threadIdx.y\n",
        "\n",
        "    bpg = cuda.gridDim.x    # blocks per grid\n",
        "\n",
        "    # Each thread computes one element in the result matrix.\n",
        "    # The dot product is chunked into dot products of TPB-long vectors.\n",
        "    tmp = 0\n",
        "    for block in range(bpg):\n",
        "        # Preload data into shared memory\n",
        "        sA[ti * TPB + tj] = a[i * size + tj + block * TPB]\n",
        "        sB[ti * TPB + tj] = b[(ti + block * TPB) * size + j]\n",
        "    \n",
        "        cuda.syncthreads()\n",
        "\n",
        "        for k in range(TPB):\n",
        "            tmp += sA[ti*TPB+k] * sB[k*TPB+tj]\n",
        "\n",
        "        # Wait until all threads finish computing\n",
        "        cuda.syncthreads()\n",
        "\n",
        "    c[i * size + j] = tmp"
      ],
      "metadata": {
        "id": "bfuD-kN0ZCAK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpmqfXo_3hbx",
        "outputId": "03e2a768-8a36-4e4c-94bd-57dceb21331f"
      },
      "source": [
        "size = 2048\n",
        "\n",
        "threadsperblock = (TPB,TPB)\n",
        "blockspergrid = (int(size/TPB),int(size/TPB))\n",
        "\n",
        "print(threadsperblock, blockspergrid)\n",
        "\n",
        "time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda_opt[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda_opt[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda_opt[blockspergrid, threadsperblock],size)\n",
        "time_and_check(mat_mul_numba_cuda_opt[blockspergrid, threadsperblock],size)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 8) (256, 256)\n",
            "Size:  2048  elapsed time:  0.693824052810669  checksum =  -3548.987077827435\n",
            "Size:  2048  elapsed time:  0.6047224998474121  checksum =  -3548.987077827435\n",
            "Size:  2048  elapsed time:  0.6046051979064941  checksum =  -3548.987077827435\n",
            "Size:  2048  elapsed time:  0.6080305576324463  checksum =  -3548.987077827435\n",
            "Size:  2048  elapsed time:  0.6098635196685791  checksum =  -3548.9900662559967\n",
            "Size:  2048  elapsed time:  0.1950995922088623  checksum =  -3548.9900662559967\n",
            "Size:  2048  elapsed time:  0.1922299861907959  checksum =  -3548.9900662559967\n",
            "Size:  2048  elapsed time:  0.19495272636413574  checksum =  -3548.9900662559967\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19495272636413574"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}